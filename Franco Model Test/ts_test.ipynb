{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4b/fm5gnnzx6qqc7t0l504grbjm0000gn/T/ipykernel_49346/1593892640.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Timestamp'] = df['Timestamp'].str.replace(' \\+0000', '')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Item_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-19 06:14:58</td>\n",
       "      <td>14.637844</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-19 06:14:59</td>\n",
       "      <td>14.637848</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-19 06:15:00</td>\n",
       "      <td>14.637866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-09-19 06:15:01</td>\n",
       "      <td>14.637893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-09-19 06:15:02</td>\n",
       "      <td>14.637924</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>2023-09-19 06:56:17</td>\n",
       "      <td>14.640038</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>2023-09-19 06:56:18</td>\n",
       "      <td>14.640041</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>2023-09-19 06:56:19</td>\n",
       "      <td>14.640044</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>2023-09-19 06:56:20</td>\n",
       "      <td>14.640047</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>2023-09-19 06:56:21</td>\n",
       "      <td>14.640050</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2486 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Timestamp   Latitude Item_ID\n",
       "0    2023-09-19 06:14:58  14.637844       1\n",
       "1    2023-09-19 06:14:59  14.637848       1\n",
       "2    2023-09-19 06:15:00  14.637866       1\n",
       "3    2023-09-19 06:15:01  14.637893       1\n",
       "4    2023-09-19 06:15:02  14.637924       1\n",
       "...                  ...        ...     ...\n",
       "2255 2023-09-19 06:56:17  14.640038       2\n",
       "2256 2023-09-19 06:56:18  14.640041       2\n",
       "2485 2023-09-19 06:56:19  14.640044       2\n",
       "2257 2023-09-19 06:56:20  14.640047       2\n",
       "2258 2023-09-19 06:56:21  14.640050       2\n",
       "\n",
       "[2486 rows x 3 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "\n",
    "df = pd.read_csv('Thesis Tracking Data 20230919.csv')\n",
    "df = df[['Timestamp', 'Latitude', 'Longitude']]\n",
    "# df = df.rename(columns={'Timestamp': 'timestamp'})\n",
    "# df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "df = df.astype({'Timestamp': 'string'})\n",
    "df['Timestamp'] = df['Timestamp'].str.replace(' \\+0000', '')\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "# df.set_index('timestamp')\n",
    "\n",
    "gap_fill_df = pd.DataFrame({\n",
    "    'Timestamp': pd.date_range(df['Timestamp'].min(), df['Timestamp'].max(), freq='1s')\n",
    "})\n",
    "\n",
    "df = pd.merge(df, gap_fill_df, on='Timestamp', how='outer')\n",
    "df = df.sort_values(by=['Timestamp'])\n",
    "df['Latitude'] = df['Latitude'].interpolate()\n",
    "df['Longitude'] = df['Longitude'].interpolate()\n",
    "\n",
    "df['Item_ID'] = '2'\n",
    "df.loc[0:2200, 'Item_ID'] = '1'\n",
    "\n",
    "# lat_ds = PandasDataset.from_long_dataframe(df, target='Latitude', item_id='Item_ID', timestamp='Timestamp', freq='S')\n",
    "#long_ds = PandasDataset.from_long_dataframe(df, target='Longitude', item_id='Item_ID', timestamp='Timestamp', freq='S')\n",
    "\n",
    "# lat_ds\n",
    "\n",
    "lat_df = df[['Timestamp', 'Latitude', 'Item_ID']]\n",
    "long_df = df[['Timestamp', 'Longitude', 'Item_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dataframe index is not uniformly spaced. If your dataframe contains data from multiple series in the same column (\"long\" format), consider constructing the dataset with `PandasDataset.from_long_dataframe` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco Model Test/ts_test.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m data\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m process_start \u001b[39m=\u001b[39m ProcessStartField()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m lat_list_ds \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(Map(process_start, lat_ds))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gluonts/dataset/pandas.py:217\u001b[0m, in \u001b[0;36mPandasDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 217\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_entries\n\u001b[1;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munchecked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gluonts/dataset/pandas.py:180\u001b[0m, in \u001b[0;36mPandasDataset._pair_to_dataentry\u001b[0;34m(self, item_id, df)\u001b[0m\n\u001b[1;32m    177\u001b[0m     df\u001b[39m.\u001b[39msort_index(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munchecked:\n\u001b[0;32m--> 180\u001b[0m     \u001b[39massert\u001b[39;00m is_uniform(df\u001b[39m.\u001b[39mindex), (\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDataframe index is not uniformly spaced. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf your dataframe contains data from multiple series in the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msame column (\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlong\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m format), consider constructing the \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    184\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdataset with `PandasDataset.from_long_dataframe` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    187\u001b[0m entry \u001b[39m=\u001b[39m {\n\u001b[1;32m    188\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m\"\u001b[39m: df\u001b[39m.\u001b[39mindex[\u001b[39m0\u001b[39m],\n\u001b[1;32m    189\u001b[0m }\n\u001b[1;32m    191\u001b[0m target \u001b[39m=\u001b[39m df[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget]\u001b[39m.\u001b[39mvalues\n",
      "\u001b[0;31mAssertionError\u001b[0m: Dataframe index is not uniformly spaced. If your dataframe contains data from multiple series in the same column (\"long\" format), consider constructing the dataset with `PandasDataset.from_long_dataframe` instead."
     ]
    }
   ],
   "source": [
    "from gluonts.itertools import Map\n",
    "\n",
    "class ProcessStartField():\n",
    "    ts_id = 0\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        data[\"start\"] = data[\"start\"].to_timestamp()\n",
    "        data[\"feat_static_cat\"] = [self.ts_id]\n",
    "        self.ts_id += 1\n",
    "        \n",
    "        return data\n",
    "\n",
    "process_start = ProcessStartField()\n",
    "\n",
    "# lat_list_ds = list(Map(process_start, lat_ds))\n",
    "# long_list_ds = list(Map(process_start, long_ds))\n",
    "\n",
    "# lat_list_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting data for work with Features\n",
    "\n",
    "lat_df['target'] = [None if x == 15 else df['Latitude'].iloc[:x].tolist() for x in range(min(len(df), 15))]\n",
    "long_df['target'] = [None if x == 15 else df['Longitude'].iloc[:x].tolist() for x in range(min(len(df), 15))]\n",
    "\n",
    "lat_df.dropna()\n",
    "long_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic behind this: each row will be used as test data, whose validation data is \n",
    "# found in the succeeding row\n",
    "# It already incorporates one loop around Line A as a starting point\n",
    "\n",
    "lat_df['target'] = [None if x < 2250 else df['Latitude'].iloc[2200:x].tolist() for x in range(len(df))]\n",
    "long_df['target'] = [None if x < 2250 else df['Longitude'].iloc[2200:x].tolist() for x in range(len(df))]\n",
    "\n",
    "lat_df.dropna()\n",
    "long_df.dropna()\n",
    "\n",
    "# To ensure that the prediction length is 10 gps coordinates long, we take a sample every 10 seconds\n",
    "lat_df = lat_df.iloc[::10, :]\n",
    "long_df = long_df.iloc[::10, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Timestamp': Timestamp('2023-09-19 06:14:58'),\n",
       " 'Latitude': 14.637844495160426,\n",
       " 'Item_ID': '1',\n",
       " '__index_level_0__': 0}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, Features, Value, Sequence\n",
    "\n",
    "features = Features(\n",
    "    {\n",
    "        'Timestamp': Value('timestamp[s]'),\n",
    "        'target': Sequence(Value('float64')),\n",
    "        'feat_static_cat': Sequence(Value('uint64')),\n",
    "        'item_id': Value('string')\n",
    "    }\n",
    ")\n",
    "\n",
    "lat_ds = Dataset.from_pandas(lat_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate accelerate gluonts ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\ntrain_dataset.set_transform(partial(transform_start_field, freq=freq))\\ntest_dataset.set_transform(partial(transform_start_field, freq=freq))\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the start period to a pandas period\n",
    "from functools import partial\n",
    "\n",
    "def convert_to_pandas_period(date, freq):\n",
    "    return pd.Period(date, freq)\n",
    "\n",
    "def transofrm_start_field(batch, freq):\n",
    "    batch['start'] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
    "test_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.time_feature import get_lags_for_frequency, time_features_from_frequency_str\n",
    "\n",
    "freq='10s'\n",
    "lags_seq = get_lags_for_frequency(freq)\n",
    "time_features = time_features_from_frequency_str(freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the transformer model\n",
    "prediction_length = 10\n",
    "\n",
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
    "\n",
    "config = TimeSeriesTransformerConfig(\n",
    "    prediction_lnegth=prediction_length,\n",
    "    context_length=2000,\n",
    "    lags_sequence=lags_seq,\n",
    "    num_time_features=len(time_features),\n",
    "    num_static_categorical_features=1,\n",
    "    cardinality=len(train_dataset),\n",
    "    embedding_dimension=[1],\n",
    "\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    d_model=32\n",
    ")\n",
    "\n",
    "model = TimeSeriesTransformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.time_feature import (\n",
    "    time_features_from_frequency_str,\n",
    "    TimeFeature,\n",
    "    get_lags_for_frequency,\n",
    ")\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    AsNumpyArray,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    RemoveFields,\n",
    "    SelectFields,\n",
    "    SetField,\n",
    "    TestSplitSampler,\n",
    "    Transformation,\n",
    "    ValidationSplitSampler,\n",
    "    VstackFeatures,\n",
    "    RenameFields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
    "    remove_field_names = []\n",
    "    if config.num_static_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
    "    if config.num_dynamic_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
    "    if config.num_static_categorical_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
    "\n",
    "    # a bit like torchvision.transforms.Compose\n",
    "    return Chain(\n",
    "        # step 1: remove static/dynamic fields if not specified\n",
    "        [RemoveFields(field_names=remove_field_names)]\n",
    "        # step 2: convert the data to NumPy (potentially not needed)\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_CAT,\n",
    "                    expected_ndim=1,\n",
    "                    dtype=int,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_REAL,\n",
    "                    expected_ndim=1,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_real_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + [\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.TARGET,\n",
    "                # we expect an extra dim for the multivariate case:\n",
    "                expected_ndim=1, ## cs: from expected_ndim=1 if config.input_size == 1 else 2,\n",
    "            ),\n",
    "            # step 3: handle the NaN's by filling in the target with zero\n",
    "            # and return the mask (which is in the observed values)\n",
    "            # true for observed values, false for nan's\n",
    "            # the decoder uses this mask (no loss is incurred for unobserved values)\n",
    "            # see loss_weights inside the xxxForPrediction model\n",
    "            AddObservedValuesIndicator(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.OBSERVED_VALUES,\n",
    "            ),\n",
    "            # step 4: add temporal features based on freq of the dataset\n",
    "            # month of year in the case when freq=\"M\"\n",
    "            # these serve as positional encodings\n",
    "            AddTimeFeatures(\n",
    "                start_field=FieldName.START,\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                time_features=time_features_from_frequency_str(freq),\n",
    "                pred_length=config.prediction_length,\n",
    "            ),\n",
    "#             # step 5: add another temporal feature (just a single number)\n",
    "#             # tells the model where in the life the value of the time series is\n",
    "#             # sort of running counter\n",
    "#             AddAgeFeature(\n",
    "#                 target_field=FieldName.TARGET,\n",
    "#                 output_field=FieldName.FEAT_AGE,\n",
    "#                 pred_length=config.prediction_length,\n",
    "#                 log_scale=True,\n",
    "#             ),\n",
    "            # step 6: vertically stack all the temporal features into the key FEAT_TIME\n",
    "            VstackFeatures(\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                input_fields=[FieldName.FEAT_TIME]  ## cs: from \"[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\"\n",
    "                + (\n",
    "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
    "                    if config.num_dynamic_real_features > 0\n",
    "                    else []\n",
    "                ),\n",
    "            ),\n",
    "            # step 7: rename to match HuggingFace names\n",
    "            RenameFields(\n",
    "                mapping={\n",
    "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
    "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
    "                    FieldName.FEAT_TIME: \"time_features\",\n",
    "                    FieldName.TARGET: \"values\",\n",
    "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# print(len(FieldName.FEAT_TIME),len(FieldName.FEAT_TIME), len(FieldName.FEAT_DYNAMIC_REAL))\n",
    "for i in [FieldName.FEAT_STATIC_CAT, FieldName.FEAT_STATIC_REAL, FieldName.FEAT_TIME, FieldName.TARGET, FieldName.OBSERVED_VALUES]:\n",
    "    print(i, len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTANCE SPLITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.transform.sampler import InstanceSampler\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def create_instance_splitter(\n",
    "    config: PretrainedConfig,\n",
    "    mode: str,\n",
    "    train_sampler: Optional[InstanceSampler] = None,\n",
    "    validation_sampler: Optional[InstanceSampler] = None,\n",
    ") -> Transformation:\n",
    "    assert mode in [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    instance_sampler = {\n",
    "        \"train\": train_sampler\n",
    "        or ExpectedNumInstanceSampler(\n",
    "            num_instances=1.0, min_future=config.prediction_length\n",
    "        ),\n",
    "        \"validation\": validation_sampler\n",
    "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
    "        \"test\": TestSplitSampler(),\n",
    "    }[mode]\n",
    "\n",
    "    return InstanceSplitter(\n",
    "        target_field=\"values\",\n",
    "        is_pad_field=FieldName.IS_PAD,\n",
    "        start_field=FieldName.START,\n",
    "        forecast_start_field=FieldName.FORECAST_START,\n",
    "        instance_sampler=instance_sampler,\n",
    "        past_length=config.context_length + max(config.lags_sequence),\n",
    "        future_length=config.prediction_length,\n",
    "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
