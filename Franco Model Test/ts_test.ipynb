{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4b/fm5gnnzx6qqc7t0l504grbjm0000gn/T/ipykernel_49346/3743159901.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Timestamp'] = df['Timestamp'].str.replace(' \\+0000', '')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "\n",
    "df = pd.read_csv('Thesis Tracking Data 20230919.csv')\n",
    "df = df[['Timestamp', 'Latitude', 'Longitude']]\n",
    "# df = df.rename(columns={'Timestamp': 'timestamp'})\n",
    "# df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "df = df.astype({'Timestamp': 'string'})\n",
    "df['Timestamp'] = df['Timestamp'].str.replace(' \\+0000', '')\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "# df.set_index('timestamp')\n",
    "\n",
    "gap_fill_df = pd.DataFrame({\n",
    "    'Timestamp': pd.date_range(df['Timestamp'].min(), df['Timestamp'].max(), freq='1s')\n",
    "})\n",
    "\n",
    "df = pd.merge(df, gap_fill_df, on='Timestamp', how='outer')\n",
    "df = df.sort_values(by=['Timestamp'])\n",
    "df['Latitude'] = df['Latitude'].interpolate()\n",
    "df['Longitude'] = df['Longitude'].interpolate()\n",
    "\n",
    "df['Item_ID'] = '2'\n",
    "df.loc[0:2200, 'Item_ID'] = '1'\n",
    "\n",
    "# lat_ds = PandasDataset.from_long_dataframe(df, target='Latitude', item_id='Item_ID', timestamp='Timestamp', freq='S')\n",
    "#long_ds = PandasDataset.from_long_dataframe(df, target='Longitude', item_id='Item_ID', timestamp='Timestamp', freq='S')\n",
    "\n",
    "# lat_ds\n",
    "\n",
    "lat_df = df[['Timestamp', 'Latitude', 'Item_ID']]\n",
    "long_df = df[['Timestamp', 'Longitude', 'Item_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.itertools import Map\n",
    "\n",
    "class ProcessStartField():\n",
    "    ts_id = 0\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        data[\"start\"] = data[\"start\"].to_timestamp()\n",
    "        data[\"feat_static_cat\"] = [self.ts_id]\n",
    "        self.ts_id += 1\n",
    "        \n",
    "        return data\n",
    "\n",
    "process_start = ProcessStartField()\n",
    "\n",
    "# lat_list_ds = list(Map(process_start, lat_ds))\n",
    "# long_list_ds = list(Map(process_start, long_ds))\n",
    "\n",
    "# lat_list_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic behind this: each row will be used as test data, whose validation data is \n",
    "# found in the succeeding row\n",
    "# It already incorporates one loop around Line A as a starting point\n",
    "\n",
    "lat_df['Target'] = [None if x < 2250 else df['Latitude'].iloc[2200:x].tolist() for x in range(len(df))]\n",
    "long_df['Target'] = [None if x < 2250 else df['Longitude'].iloc[2200:x].tolist() for x in range(len(df))]\n",
    "\n",
    "# To ensure that the prediction length is 10 gps coordinates long, we take a sample every 10 seconds\n",
    "lat_df = lat_df.iloc[::10, :]\n",
    "long_df = long_df.iloc[::10, :]\n",
    "\n",
    "lat_df = lat_df[['Timestamp', 'Item_ID', 'Target']]\n",
    "long_df = long_df[['Timestamp', 'Item_ID', 'Target']]\n",
    "\n",
    "lat_df['feat_static_cat'] = [[0]] * len(lat_df)\n",
    "long_df['feat_static_cat'] = [[0]] * len(long_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Value, Sequence\n",
    "\n",
    "features = Features(\n",
    "    {\n",
    "        'Timestamp': Value('timestamp[s]'),\n",
    "        'Target': Sequence(Value('float64')),\n",
    "        'feat_static_cat': Sequence(Value('uint64')),\n",
    "        'Item_ID': Value('string')\n",
    "    }\n",
    ")\n",
    "\n",
    "lat_df.dropna(axis=0, how='any', inplace=True)\n",
    "long_df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "lat_ds = Dataset.from_pandas(lat_df)\n",
    "long_ds = Dataset.from_pandas(long_df)\n",
    "\n",
    "lat_df_test = lat_df.copy()\n",
    "long_df_test = long_df.copy()\n",
    "\n",
    "lat_df_test['Target'] = lat_df_test['Target'].shift(-1)\n",
    "long_df_test['Target'] = lat_df_test['Target'].shift(-1)\n",
    "\n",
    "lat_df_test.dropna(axis=0, how='any', inplace=True)\n",
    "long_df_test.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "lat_test_ds = Dataset.from_pandas(lat_df_test)\n",
    "long_test_ds = Dataset.from_pandas(long_df_test)\n",
    "\n",
    "lat_dataset = {'train': lat_ds, 'test': lat_test_ds}\n",
    "long_dataset = {'train': long_ds, 'test': long_test_ds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.33.2)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.14.5)\n",
      "Requirement already satisfied: evaluate in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: accelerate in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: gluonts in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.13.5)\n",
      "Requirement already satisfied: ujson in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (5.8.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: psutil in /Users/franco/Library/Python/3.10/lib/python/site-packages (from accelerate) (5.9.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: pydantic~=1.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gluonts) (1.10.12)\n",
      "Requirement already satisfied: toolz~=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gluonts) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gluonts) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/franco/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install transformers datasets evaluate accelerate gluonts ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the start period to a pandas period\n",
    "from functools import partial\n",
    "\n",
    "def convert_to_pandas_period(date, freq):\n",
    "    return pd.Period(date, freq)\n",
    "\n",
    "def transform_start_field(batch, freq):\n",
    "    batch['start'] = [convert_to_pandas_period(date, freq) for date in batch[\"Timestamp\"]]\n",
    "\n",
    "freq='10s'\n",
    "\n",
    "lat_dataset['train'].set_transform(partial(transform_start_field, freq=freq))\n",
    "lat_dataset['test'].set_transform(partial(transform_start_field, freq=freq))\n",
    "\n",
    "long_dataset['train'].set_transform(partial(transform_start_field, freq=freq))\n",
    "long_dataset['test'].set_transform(partial(transform_start_field, freq=freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Timestamp', 'Item_ID', 'Target', 'feat_static_cat', '__index_level_0__'],\n",
      "    num_rows: 24\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from gluonts.time_feature import get_lags_for_frequency, time_features_from_frequency_str\n",
    "\n",
    "lags_seq = get_lags_for_frequency(freq)\n",
    "time_features = time_features_from_frequency_str(freq)\n",
    "\n",
    "print(lat_dataset['train'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the transformer model\n",
    "prediction_length = 10\n",
    "\n",
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
    "\n",
    "lat_config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=2000,\n",
    "    lags_sequence=lags_seq,\n",
    "    num_time_features=len(time_features),\n",
    "    num_static_categorical_features=1,\n",
    "    cardinality=[len(lat_dataset['train'])],\n",
    "    embedding_dimension=[1],\n",
    "\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    d_model=32\n",
    ")\n",
    "\n",
    "long_config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=2000,\n",
    "    lags_sequence=lags_seq,\n",
    "    num_time_features=len(time_features),\n",
    "    num_static_categorical_features=1,\n",
    "    cardinality=[len(long_dataset['train'])],\n",
    "    embedding_dimension=[1],\n",
    "\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    d_model=32\n",
    ")\n",
    "\n",
    "lat_model = TimeSeriesTransformerForPrediction(lat_config)\n",
    "long_model = TimeSeriesTransformerForPrediction(long_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.time_feature import (\n",
    "    time_features_from_frequency_str,\n",
    "    TimeFeature,\n",
    "    get_lags_for_frequency,\n",
    ")\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    AsNumpyArray,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    RemoveFields,\n",
    "    SelectFields,\n",
    "    SetField,\n",
    "    TestSplitSampler,\n",
    "    Transformation,\n",
    "    ValidationSplitSampler,\n",
    "    VstackFeatures,\n",
    "    RenameFields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_static_cat 15\n",
      "feat_static_real 16\n",
      "time_feat 9\n",
      "target 6\n",
      "observed_values 15\n"
     ]
    }
   ],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
    "    remove_field_names = []\n",
    "    if config.num_static_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
    "    if config.num_dynamic_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
    "    if config.num_static_categorical_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
    "\n",
    "    # a bit like torchvision.transforms.Compose\n",
    "    return Chain(\n",
    "        # step 1: remove static/dynamic fields if not specified\n",
    "        [RemoveFields(field_names=remove_field_names)]\n",
    "        # step 2: convert the data to NumPy (potentially not needed)\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_CAT,\n",
    "                    expected_ndim=1,\n",
    "                    dtype=int,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_REAL,\n",
    "                    expected_ndim=1,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_real_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + [\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.TARGET,\n",
    "                # we expect an extra dim for the multivariate case:\n",
    "                expected_ndim=1, ## cs: from expected_ndim=1 if config.input_size == 1 else 2,\n",
    "            ),\n",
    "            # step 3: handle the NaN's by filling in the target with zero\n",
    "            # and return the mask (which is in the observed values)\n",
    "            # true for observed values, false for nan's\n",
    "            # the decoder uses this mask (no loss is incurred for unobserved values)\n",
    "            # see loss_weights inside the xxxForPrediction model\n",
    "            AddObservedValuesIndicator(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.OBSERVED_VALUES,\n",
    "            ),\n",
    "            # step 4: add temporal features based on freq of the dataset\n",
    "            # month of year in the case when freq=\"M\"\n",
    "            # these serve as positional encodings\n",
    "            AddTimeFeatures(\n",
    "                start_field=FieldName.START,\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                time_features=time_features_from_frequency_str(freq),\n",
    "                pred_length=config.prediction_length,\n",
    "            ),\n",
    "#             # step 5: add another temporal feature (just a single number)\n",
    "#             # tells the model where in the life the value of the time series is\n",
    "#             # sort of running counter\n",
    "#             AddAgeFeature(\n",
    "#                 target_field=FieldName.TARGET,\n",
    "#                 output_field=FieldName.FEAT_AGE,\n",
    "#                 pred_length=config.prediction_length,\n",
    "#                 log_scale=True,\n",
    "#             ),\n",
    "            # step 6: vertically stack all the temporal features into the key FEAT_TIME\n",
    "            VstackFeatures(\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                input_fields=[FieldName.FEAT_TIME]  ## cs: from \"[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\"\n",
    "                + (\n",
    "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
    "                    if config.num_dynamic_real_features > 0\n",
    "                    else []\n",
    "                ),\n",
    "            ),\n",
    "            # step 7: rename to match HuggingFace names\n",
    "            RenameFields(\n",
    "                mapping={\n",
    "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
    "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
    "                    FieldName.FEAT_TIME: \"time_features\",\n",
    "                    FieldName.TARGET: \"values\",\n",
    "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# print(len(FieldName.FEAT_TIME),len(FieldName.FEAT_TIME), len(FieldName.FEAT_DYNAMIC_REAL))\n",
    "for i in [FieldName.FEAT_STATIC_CAT, FieldName.FEAT_STATIC_REAL, FieldName.FEAT_TIME, FieldName.TARGET, FieldName.OBSERVED_VALUES]:\n",
    "    print(i, len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTANCE SPLITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.transform.sampler import InstanceSampler\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def create_instance_splitter(\n",
    "    config: PretrainedConfig,\n",
    "    mode: str,\n",
    "    train_sampler: Optional[InstanceSampler] = None,\n",
    "    validation_sampler: Optional[InstanceSampler] = None,\n",
    ") -> Transformation:\n",
    "    assert mode in [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    instance_sampler = {\n",
    "        \"train\": train_sampler\n",
    "        or ExpectedNumInstanceSampler(\n",
    "            num_instances=1.0, min_future=config.prediction_length\n",
    "        ),\n",
    "        \"validation\": validation_sampler\n",
    "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
    "        \"test\": TestSplitSampler(),\n",
    "    }[mode]\n",
    "\n",
    "    return InstanceSplitter(\n",
    "        target_field=\"values\",\n",
    "        is_pad_field=FieldName.IS_PAD,\n",
    "        start_field=FieldName.START,\n",
    "        forecast_start_field=FieldName.FORECAST_START,\n",
    "        instance_sampler=instance_sampler,\n",
    "        past_length=config.context_length + max(config.lags_sequence),\n",
    "        future_length=config.prediction_length,\n",
    "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_lightning in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.0.9)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (1.21.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (1.13.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (2023.6.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (4.4.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (0.9.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=17.1->pytorch_lightning) (3.0.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/franco/Library/Python/3.10/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from torch.utils.data import IterableDataset, TransformedDataset\n",
    "\n",
    "class CustomIterableDataset(IterableDataset):\n",
    "    def __init__(self, transform_dataset: TransformedDataset):\n",
    "        self.transformed_dataset = transformed_dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        for e in self.transformed_dataset:\n",
    "            yield e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.itertools import Cyclic, IterableSlice, PseudoShuffled\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def create_train_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    num_batches_per_epoch: int,\n",
    "    shuffle_buffer_length: Optional[int] = None,\n",
    "    **kwargs,\n",
    ") -> Iterable:\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
    "        \"future_values\",\n",
    "        \"future_observed_mask\",\n",
    "    ]\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=True)\n",
    "\n",
    "    # we initialize a Training instance\n",
    "    instance_splitter = create_instance_splitter(config, \"train\") + SelectFields(\n",
    "        TRAINING_INPUT_NAMES\n",
    "    )\n",
    "\n",
    "    # the instance splitter will sample a window of\n",
    "    # context length + lags + prediction length (from the 366 possible transformed time series)\n",
    "    # randomly from within the target time series and return an iterator.\n",
    "    training_instances = instance_splitter.apply(\n",
    "        Cyclic(transformed_data)\n",
    "        if shuffle_buffer_length is None\n",
    "        else PseudoShuffled(\n",
    "            Cyclic(transformed_data),\n",
    "            shuffle_buffer_length=shuffle_buffer_length,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # from the training instances iterator we now return a Dataloader which will\n",
    "    # continue to sample random windows for as long as it is called\n",
    "    # to return batch_size of the appropriate tensors ready for training!\n",
    "    return IterableSlice(\n",
    "        iter(\n",
    "            DataLoader(\n",
    "                CustomIterableDataset(training_instances, len(FieldName.TARGET)),\n",
    "                batch_size=batch_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "        ),\n",
    "        num_batches_per_epoch,\n",
    "    )\n",
    "\n",
    "def create_test_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=False)\n",
    "\n",
    "    # we create a Test Instance splitter which will sample the very last\n",
    "    # context window seen during training only for the encoder.\n",
    "    instance_sampler = create_instance_splitter(config, \"test\") + SelectFields(\n",
    "        PREDICTION_INPUT_NAMES\n",
    "    )\n",
    "\n",
    "    # we apply the transformations in test mode\n",
    "    testing_instances = instance_sampler.apply(transformed_data, is_train=False)\n",
    "\n",
    "    # This returns a Dataloader which will go over the dataset once.\n",
    "    return DataLoader(\n",
    "        CustomIterableDataset(testing_instances, len(FieldName.TARGET)), \n",
    "        batch_size=batch_size, \n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating Dataloaders\n",
    "batch_size = 256\n",
    "\n",
    "lat_train_dataloader = create_train_dataloader(\n",
    "    config=lat_config,\n",
    "    freq=freq,\n",
    "    data=lat_dataset['train'],\n",
    "    batch_size=batch_size,\n",
    "    num_batches_per_epoch=100,\n",
    ")\n",
    "\n",
    "long_train_dataloader = create_train_dataloader(\n",
    "    config=long_config,\n",
    "    freq=freq,\n",
    "    data=long_dataset['train'],\n",
    "    batch_size=batch_size,\n",
    "    num_batches_per_epoch=100,\n",
    ")\n",
    "\n",
    "lat_test_dataloader = create_test_dataloader(\n",
    "    config=lat_config,\n",
    "    freq=freq,\n",
    "    data=lat_dataset['test'],\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "long_test_dataloader = create_test_dataloader(\n",
    "    config=long_config,\n",
    "    freq=freq,\n",
    "    data=long_dataset['test'],\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'gluonts.transform._base.TransformedDataset'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco Model Test/ts_test.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m## cs\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m lat_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(lat_train_dataloader))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m long_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(long_train_dataloader))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# perform forward pass\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gluonts/itertools.py:241\u001b[0m, in \u001b[0;36mIterableSlice.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 241\u001b[0m     \u001b[39myield from\u001b[39;00m itertools\u001b[39m.\u001b[39mislice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterable, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_iter)\n\u001b[0;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:151\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m             \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    149\u001b[0m             \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[0;32m--> 151\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'gluonts.transform._base.TransformedDataset'>"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") ## cs\n",
    "print(device)\n",
    "\n",
    "lat_batch = next(iter(lat_train_dataloader))\n",
    "long_batch = next(iter(long_train_dataloader))\n",
    "\n",
    "# perform forward pass\n",
    "lat_outputs = lat_model(\n",
    "    past_values=lat_batch[\"past_values\"],\n",
    "    past_time_features=lat_batch[\"past_time_features\"],\n",
    "    past_observed_mask=lat_batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=lat_batch[\"static_categorical_features\"]\n",
    "    if lat_config.num_static_categorical_features > 0\n",
    "    else None,\n",
    "    static_real_features=lat_batch[\"static_real_features\"]\n",
    "    if lat_config.num_static_real_features > 0\n",
    "    else torch.zeros((batch_size, 0), dtype=torch.float32, device=device),  ## cs: changed from \"else None\"\n",
    "    future_values=lat_batch[\"future_values\"],\n",
    "    future_time_features=lat_batch[\"future_time_features\"],\n",
    "    future_observed_mask=lat_batch[\"future_observed_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "long_outputs = long_model(\n",
    "    past_values=long_batch[\"past_values\"],\n",
    "    past_time_features=long_batch[\"past_time_features\"],\n",
    "    past_observed_mask=long_batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=long_batch[\"static_categorical_features\"]\n",
    "    if long_config.num_static_categorical_features > 0\n",
    "    else None,\n",
    "    static_real_features=long_batch[\"static_real_features\"]\n",
    "    if long_config.num_static_real_features > 0\n",
    "    else torch.zeros((batch_size, 0), dtype=torch.float32, device=device),  ## cs: changed from \"else None\"\n",
    "    future_values=long_batch[\"future_values\"],\n",
    "    future_time_features=long_batch[\"future_time_features\"],\n",
    "    future_observed_mask=long_batch[\"future_observed_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "\n",
    "lat_accelerator = Accelerator()\n",
    "lat_device = lat_accelerator.device\n",
    "lat_device = \"cpu\"  ## cs\n",
    "\n",
    "lat_model.to(lat_device)\n",
    "long_optimizer = AdamW(lat_model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\n",
    "\n",
    "# model, optimizer, train_dataloader = accelerator.prepare(\n",
    "#     model,\n",
    "#     optimizer,\n",
    "#     train_dataloader,\n",
    "# )\n",
    "\n",
    "lat_model.train()\n",
    "for epoch in range(40):\n",
    "    for idx, batch in enumerate(lat_train_dataloader):\n",
    "        long_optimizer.zero_grad()\n",
    "        outputs = lat_model(\n",
    "            static_categorical_features=batch[\"static_categorical_features\"].to(lat_device)\n",
    "            if lat_config.num_static_categorical_features > 0\n",
    "            else None,\n",
    "            static_real_features=batch[\"static_real_features\"].to(lat_device)\n",
    "            if lat_config.num_static_real_features > 0\n",
    "            else torch.zeros((batch_size, 0), dtype=torch.float32, device=lat_device).to(lat_device),  ## cs: changed from \"else None\"\n",
    "            past_time_features=batch[\"past_time_features\"].to(lat_device),\n",
    "            past_values=batch[\"past_values\"].to(lat_device),\n",
    "            future_time_features=batch[\"future_time_features\"].to(lat_device),\n",
    "            future_values=batch[\"future_values\"].to(lat_device),\n",
    "            past_observed_mask=batch[\"past_observed_mask\"].to(lat_device),\n",
    "            future_observed_mask=batch[\"future_observed_mask\"].to(lat_device),\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        lat_accelerator.backward(loss)\n",
    "        long_optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitude Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "\n",
    "long_accelerator = Accelerator()\n",
    "long_device = long_accelerator.device\n",
    "long_device = \"cpu\"  ## cs\n",
    "\n",
    "long_model.to(device)\n",
    "long_optimizer = AdamW(long_model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\n",
    "\n",
    "# model, optimizer, train_dataloader = accelerator.prepare(\n",
    "#     model,\n",
    "#     optimizer,\n",
    "#     train_dataloader,\n",
    "# )\n",
    "\n",
    "long_model.train()\n",
    "for epoch in range(40):\n",
    "    for idx, batch in enumerate(long_train_dataloader):\n",
    "        long_optimizer.zero_grad()\n",
    "        outputs = long_model(\n",
    "            static_categorical_features=batch[\"static_categorical_features\"].to(long_device)\n",
    "            if long_config.num_static_categorical_features > 0\n",
    "            else None,\n",
    "            static_real_features=batch[\"static_real_features\"].to(long_device)\n",
    "            if long_config.num_static_real_features > 0\n",
    "            else torch.zeros((batch_size, 0), dtype=torch.float32, device=long_device).to(long_device),  ## cs: changed from \"else None\"\n",
    "            past_time_features=batch[\"past_time_features\"].to(long_device),\n",
    "            past_values=batch[\"past_values\"].to(long_device),\n",
    "            future_time_features=batch[\"future_time_features\"].to(long_device),\n",
    "            future_values=batch[\"future_values\"].to(long_device),\n",
    "            past_observed_mask=batch[\"past_observed_mask\"].to(long_device),\n",
    "            future_observed_mask=batch[\"future_observed_mask\"].to(long_device),\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        long_accelerator.backward(loss)\n",
    "        long_optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_model.eval()\n",
    "\n",
    "lat_forecasts = []\n",
    "\n",
    "for batch in lat_test_dataloader:\n",
    "    outputs = lat_model.generate(\n",
    "        static_categorical_features=batch[\"static_categorical_features\"].to(lat_device)\n",
    "        if lat_config.num_static_categorical_features > 0\n",
    "        else None,\n",
    "        static_real_features=batch[\"static_real_features\"].to(lat_device)\n",
    "        if lat_config.num_static_real_features > 0\n",
    "        else torch.zeros((3, 0), dtype=torch.float32, device=lat_device).to(lat_device),  ## cs: changed from \"else None\"\n",
    "        past_time_features=batch[\"past_time_features\"].to(lat_device),\n",
    "        past_values=batch[\"past_values\"].to(lat_device),\n",
    "        future_time_features=batch[\"future_time_features\"].to(lat_device),\n",
    "        past_observed_mask=batch[\"past_observed_mask\"].to(lat_device),\n",
    "    )\n",
    "    lat_forecasts.append(outputs.sequences.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/franco/Library/Python/3.10/lib/python/site-packages (from scikit-learn) (1.9.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lat_forecasts = np.vstack(lat_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "lat_mase_metric = load(\"evaluate-metric/mase\")\n",
    "lat_smape_metric = load(\"evaluate-metric/smape\")\n",
    "lat_mse_metric = load(\"evaluate-metric/mse\")\n",
    "\n",
    "lat_forecast_median = np.median(lat_forecasts, 1)\n",
    "\n",
    "lat_mase_metrics = []\n",
    "lat_smape_metrics = []\n",
    "lat_mse_metrics = []\n",
    "for item_id, ts in enumerate(lat_dataset['test']):\n",
    "    training_data = ts[\"target\"][:-prediction_length]\n",
    "    ground_truth = ts[\"target\"][-prediction_length:]\n",
    "#     mase = mase_metric.compute(   cs: commented out cuz not working\n",
    "#         predictions=forecast_median[item_id],\n",
    "#         references=np.array(ground_truth),\n",
    "#         training=np.array(training_data),\n",
    "#         periodicity=get_seasonality(freq),\n",
    "#     )\n",
    "          \n",
    "#     mase_metrics.append(mase[\"mase\"])\n",
    "    print(lat_forecast_median[item_id])\n",
    "    print(np.array(ground_truth))\n",
    "\n",
    "    long_smape = lat_smape_metric.compute(\n",
    "        predictions=lat_forecast_median[item_id],\n",
    "        references=np.array(ground_truth),\n",
    "    )\n",
    "    lat_smape_metrics.append(long_smape[\"smape\"])\n",
    "    \n",
    "    lat_mse = lat_mse_metric.compute(predictions=lat_forecast_median[item_id], references=np.array(ground_truth))  ## cs: added for mse\n",
    "    lat_mse_metrics.append(lat_mse[\"mse\"])\n",
    "\n",
    "print(f\"sMAPE: {np.mean(lat_smape_metrics)}, MSE: {np.mean(lat_mse_metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitude Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_model.eval()\n",
    "\n",
    "long_forecasts = []\n",
    "\n",
    "for batch in long_test_dataloader:\n",
    "    outputs = long_model.generate(\n",
    "        static_categorical_features=batch[\"static_categorical_features\"].to(long_device)\n",
    "        if long_config.num_static_categorical_features > 0\n",
    "        else None,\n",
    "        static_real_features=batch[\"static_real_features\"].to(long_device)\n",
    "        if long_config.num_static_real_features > 0\n",
    "        else torch.zeros((3, 0), dtype=torch.float32, device=long_device).to(long_device),  ## cs: changed from \"else None\"\n",
    "        past_time_features=batch[\"past_time_features\"].to(long_device),\n",
    "        past_values=batch[\"past_values\"].to(long_device),\n",
    "        future_time_features=batch[\"future_time_features\"].to(long_device),\n",
    "        past_observed_mask=batch[\"past_observed_mask\"].to(long_device),\n",
    "    )\n",
    "    long_forecasts.append(outputs.sequences.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "long_forecasts = np.vstack(long_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "long_mase_metric = load(\"evaluate-metric/mase\")\n",
    "long_smape_metric = load(\"evaluate-metric/smape\")\n",
    "long_mse_metric = load(\"evaluate-metric/mse\")\n",
    "\n",
    "long_forecast_median = np.median(lat_forecasts, 1)\n",
    "\n",
    "long_mase_metrics = []\n",
    "long_smape_metrics = []\n",
    "long_mse_metrics = []\n",
    "for item_id, ts in enumerate(long_dataset['test']):\n",
    "    training_data = ts[\"target\"][:-prediction_length]\n",
    "    ground_truth = ts[\"target\"][-prediction_length:]\n",
    "#     mase = mase_metric.compute(   cs: commented out cuz not working\n",
    "#         predictions=forecast_median[item_id],\n",
    "#         references=np.array(ground_truth),\n",
    "#         training=np.array(training_data),\n",
    "#         periodicity=get_seasonality(freq),\n",
    "#     )\n",
    "          \n",
    "#     mase_metrics.append(mase[\"mase\"])\n",
    "    print(long_forecast_median[item_id])\n",
    "    print(np.array(ground_truth))\n",
    "\n",
    "    long_smape = lat_smape_metric.compute(\n",
    "        predictions=lat_forecast_median[item_id],\n",
    "        references=np.array(ground_truth),\n",
    "    )\n",
    "    lat_smape_metrics.append(long_smape[\"smape\"])\n",
    "    \n",
    "    long_mse = long_mse_metric.compute(predictions=long_forecast_median[item_id], references=np.array(ground_truth))  ## cs: added for mse\n",
    "    long_mse_metrics.append(long_mse[\"mse\"])\n",
    "\n",
    "print(f\"sMAPE: {np.mean(long_smape_metrics)}, MSE: {np.mean(long_mse_metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "def plot(ts_index, dataset, forecasts):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    index = pd.period_range(\n",
    "        start=dataset[ts_index][FieldName.START],\n",
    "        periods=len(dataset[ts_index][FieldName.TARGET]),\n",
    "        freq=freq,\n",
    "    ).to_timestamp()\n",
    "\n",
    "    # Major ticks every half year, minor ticks every month,\n",
    "#     ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 7)))  ## cs: removed month axis ticks to default as auto\n",
    "#     ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "\n",
    "\n",
    "#     ax.plot(  ## cs: replaced \"-2*prediction_length\" by \"0\" to show full time graph\n",
    "#         index[-2 * prediction_length :],\n",
    "#         test_dataset[ts_index][\"target\"][-2 * prediction_length :],\n",
    "#         label=\"actual\",\n",
    "#     )\n",
    "    ax.plot(\n",
    "        index[0 :],\n",
    "        dataset[ts_index][\"target\"][0 :],\n",
    "        label=\"actual\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        index[-prediction_length:],\n",
    "        np.median(forecasts[ts_index], axis=0),\n",
    "        label=\"median\",\n",
    "    )\n",
    "\n",
    "    plt.fill_between(\n",
    "        index[-prediction_length:],\n",
    "        forecasts[ts_index].mean(0) - forecasts[ts_index].std(axis=0),\n",
    "        forecasts[ts_index].mean(0) + forecasts[ts_index].std(axis=0),\n",
    "        alpha=0.3,\n",
    "        interpolate=True,\n",
    "        label=\"+/- 1-std\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#Latitude Plot\n",
    "for i in range(3):\n",
    "    plot(i, lat_dataset['test'], lat_forecasts)\n",
    "\n",
    "#Longitude Plot\n",
    "for i in range(3):\n",
    "    plot(i, long_dataset['test'], long_forecasts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
