{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4b/fm5gnnzx6qqc7t0l504grbjm0000gn/T/ipykernel_49346/3743159901.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Timestamp'] = df['Timestamp'].str.replace(' \\+0000', '')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "\n",
    "df = pd.read_csv('Thesis Tracking Data 20230919.csv')\n",
    "df = df[['Timestamp', 'Latitude', 'Longitude']]\n",
    "# df = df.rename(columns={'Timestamp': 'timestamp'})\n",
    "# df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "df = df.astype({'Timestamp': 'string'})\n",
    "df['Timestamp'] = df['Timestamp'].str.replace(' \\+0000', '')\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "# df.set_index('timestamp')\n",
    "\n",
    "gap_fill_df = pd.DataFrame({\n",
    "    'Timestamp': pd.date_range(df['Timestamp'].min(), df['Timestamp'].max(), freq='1s')\n",
    "})\n",
    "\n",
    "df = pd.merge(df, gap_fill_df, on='Timestamp', how='outer')\n",
    "df = df.sort_values(by=['Timestamp'])\n",
    "df['Latitude'] = df['Latitude'].interpolate()\n",
    "df['Longitude'] = df['Longitude'].interpolate()\n",
    "\n",
    "df['Item_ID'] = '2'\n",
    "df.loc[0:2200, 'Item_ID'] = '1'\n",
    "\n",
    "# lat_ds = PandasDataset.from_long_dataframe(df, target='Latitude', item_id='Item_ID', timestamp='Timestamp', freq='S')\n",
    "#long_ds = PandasDataset.from_long_dataframe(df, target='Longitude', item_id='Item_ID', timestamp='Timestamp', freq='S')\n",
    "\n",
    "# lat_ds\n",
    "\n",
    "lat_df = df[['Timestamp', 'Latitude', 'Item_ID']]\n",
    "long_df = df[['Timestamp', 'Longitude', 'Item_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dataframe index is not uniformly spaced. If your dataframe contains data from multiple series in the same column (\"long\" format), consider constructing the dataset with `PandasDataset.from_long_dataframe` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco Model Test/ts_test.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m data\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m process_start \u001b[39m=\u001b[39m ProcessStartField()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m lat_list_ds \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(Map(process_start, lat_ds))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gluonts/dataset/pandas.py:217\u001b[0m, in \u001b[0;36mPandasDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 217\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_entries\n\u001b[1;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munchecked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gluonts/dataset/pandas.py:180\u001b[0m, in \u001b[0;36mPandasDataset._pair_to_dataentry\u001b[0;34m(self, item_id, df)\u001b[0m\n\u001b[1;32m    177\u001b[0m     df\u001b[39m.\u001b[39msort_index(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munchecked:\n\u001b[0;32m--> 180\u001b[0m     \u001b[39massert\u001b[39;00m is_uniform(df\u001b[39m.\u001b[39mindex), (\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDataframe index is not uniformly spaced. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf your dataframe contains data from multiple series in the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msame column (\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlong\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m format), consider constructing the \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    184\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdataset with `PandasDataset.from_long_dataframe` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    187\u001b[0m entry \u001b[39m=\u001b[39m {\n\u001b[1;32m    188\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m\"\u001b[39m: df\u001b[39m.\u001b[39mindex[\u001b[39m0\u001b[39m],\n\u001b[1;32m    189\u001b[0m }\n\u001b[1;32m    191\u001b[0m target \u001b[39m=\u001b[39m df[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget]\u001b[39m.\u001b[39mvalues\n",
      "\u001b[0;31mAssertionError\u001b[0m: Dataframe index is not uniformly spaced. If your dataframe contains data from multiple series in the same column (\"long\" format), consider constructing the dataset with `PandasDataset.from_long_dataframe` instead."
     ]
    }
   ],
   "source": [
    "from gluonts.itertools import Map\n",
    "\n",
    "class ProcessStartField():\n",
    "    ts_id = 0\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        data[\"start\"] = data[\"start\"].to_timestamp()\n",
    "        data[\"feat_static_cat\"] = [self.ts_id]\n",
    "        self.ts_id += 1\n",
    "        \n",
    "        return data\n",
    "\n",
    "process_start = ProcessStartField()\n",
    "\n",
    "# lat_list_ds = list(Map(process_start, lat_ds))\n",
    "# long_list_ds = list(Map(process_start, long_ds))\n",
    "\n",
    "# lat_list_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic behind this: each row will be used as test data, whose validation data is \n",
    "# found in the succeeding row\n",
    "# It already incorporates one loop around Line A as a starting point\n",
    "\n",
    "lat_df['Target'] = [None if x < 2250 else df['Latitude'].iloc[2200:x].tolist() for x in range(len(df))]\n",
    "long_df['Target'] = [None if x < 2250 else df['Longitude'].iloc[2200:x].tolist() for x in range(len(df))]\n",
    "\n",
    "# To ensure that the prediction length is 10 gps coordinates long, we take a sample every 10 seconds\n",
    "lat_df = lat_df.iloc[::10, :]\n",
    "long_df = long_df.iloc[::10, :]\n",
    "\n",
    "lat_df = lat_df[['Timestamp', 'Item_ID', 'Target']]\n",
    "long_df = long_df[['Timestamp', 'Item_ID', 'Target']]\n",
    "\n",
    "lat_df['feat_static_cat'] = [[0]] * len(lat_df)\n",
    "long_df['feat_static_cat'] = [[0]] * len(long_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Value, Sequence\n",
    "\n",
    "features = Features(\n",
    "    {\n",
    "        'Timestamp': Value('timestamp[s]'),\n",
    "        'Target': Sequence(Value('float64')),\n",
    "        'feat_static_cat': Sequence(Value('uint64')),\n",
    "        'Item_ID': Value('string')\n",
    "    }\n",
    ")\n",
    "\n",
    "lat_df.dropna(axis=0, how='any', inplace=True)\n",
    "long_df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "lat_ds = Dataset.from_pandas(lat_df)\n",
    "long_ds = Dataset.from_pandas(long_df)\n",
    "\n",
    "lat_df_test = lat_df.copy()\n",
    "long_df_test = long_df.copy()\n",
    "\n",
    "lat_df_test['Target'] = lat_df_test['Target'].shift(-1)\n",
    "long_df_test['Target'] = lat_df_test['Target'].shift(-1)\n",
    "\n",
    "lat_df_test.dropna(axis=0, how='any', inplace=True)\n",
    "long_df_test.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "lat_test_ds = Dataset.from_pandas(lat_df_test)\n",
    "long_test_ds = Dataset.from_pandas(long_df_test)\n",
    "\n",
    "lat_dataset = {'train': lat_ds, 'test': lat_test_ds}\n",
    "long_dataset = {'train': long_ds, 'test': long_test_ds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.33.2)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.14.5)\n",
      "Requirement already satisfied: evaluate in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: accelerate in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: gluonts in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.13.5)\n",
      "Requirement already satisfied: ujson in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (5.8.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: psutil in /Users/franco/Library/Python/3.10/lib/python/site-packages (from accelerate) (5.9.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: pydantic~=1.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gluonts) (1.10.12)\n",
      "Requirement already satisfied: toolz~=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gluonts) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gluonts) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/franco/Library/Python/3.10/lib/python/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install transformers datasets evaluate accelerate gluonts ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the start period to a pandas period\n",
    "from functools import partial\n",
    "\n",
    "def convert_to_pandas_period(date, freq):\n",
    "    return pd.Period(date, freq)\n",
    "\n",
    "def transform_start_field(batch, freq):\n",
    "    batch['start'] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\n",
    "\n",
    "freq='10s'\n",
    "\n",
    "lat_dataset['train'].set_transform(partial(transform_start_field, freq=freq))\n",
    "lat_dataset['test'].set_transform(partial(transform_start_field, freq=freq))\n",
    "\n",
    "long_dataset['train'].set_transform(partial(transform_start_field, freq=freq))\n",
    "long_dataset['test'].set_transform(partial(transform_start_field, freq=freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Timestamp', 'Item_ID', 'Target', 'feat_static_cat', '__index_level_0__'],\n",
      "    num_rows: 24\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from gluonts.time_feature import get_lags_for_frequency, time_features_from_frequency_str\n",
    "\n",
    "lags_seq = get_lags_for_frequency(freq)\n",
    "time_features = time_features_from_frequency_str(freq)\n",
    "\n",
    "print(lat_dataset['train'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the transformer model\n",
    "prediction_length = 10\n",
    "\n",
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
    "\n",
    "lat_config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=2000,\n",
    "    lags_sequence=lags_seq,\n",
    "    num_time_features=len(time_features),\n",
    "    num_static_categorical_features=1,\n",
    "    cardinality=[len(lat_dataset['train'])],\n",
    "    embedding_dimension=[1],\n",
    "\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    d_model=32\n",
    ")\n",
    "\n",
    "long_config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=2000,\n",
    "    lags_sequence=lags_seq,\n",
    "    num_time_features=len(time_features),\n",
    "    num_static_categorical_features=1,\n",
    "    cardinality=[len(long_dataset['train'])],\n",
    "    embedding_dimension=[1],\n",
    "\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    d_model=32\n",
    ")\n",
    "\n",
    "lat_model = TimeSeriesTransformerForPrediction(lat_config)\n",
    "long_model = TimeSeriesTransformerForPrediction(long_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.time_feature import (\n",
    "    time_features_from_frequency_str,\n",
    "    TimeFeature,\n",
    "    get_lags_for_frequency,\n",
    ")\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    AsNumpyArray,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    RemoveFields,\n",
    "    SelectFields,\n",
    "    SetField,\n",
    "    TestSplitSampler,\n",
    "    Transformation,\n",
    "    ValidationSplitSampler,\n",
    "    VstackFeatures,\n",
    "    RenameFields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_static_cat 15\n",
      "feat_static_real 16\n",
      "time_feat 9\n",
      "target 6\n",
      "observed_values 15\n"
     ]
    }
   ],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
    "    remove_field_names = []\n",
    "    if config.num_static_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
    "    if config.num_dynamic_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
    "    if config.num_static_categorical_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
    "\n",
    "    # a bit like torchvision.transforms.Compose\n",
    "    return Chain(\n",
    "        # step 1: remove static/dynamic fields if not specified\n",
    "        [RemoveFields(field_names=remove_field_names)]\n",
    "        # step 2: convert the data to NumPy (potentially not needed)\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_CAT,\n",
    "                    expected_ndim=1,\n",
    "                    dtype=int,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_REAL,\n",
    "                    expected_ndim=1,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_real_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + [\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.TARGET,\n",
    "                # we expect an extra dim for the multivariate case:\n",
    "                expected_ndim=1, ## cs: from expected_ndim=1 if config.input_size == 1 else 2,\n",
    "            ),\n",
    "            # step 3: handle the NaN's by filling in the target with zero\n",
    "            # and return the mask (which is in the observed values)\n",
    "            # true for observed values, false for nan's\n",
    "            # the decoder uses this mask (no loss is incurred for unobserved values)\n",
    "            # see loss_weights inside the xxxForPrediction model\n",
    "            AddObservedValuesIndicator(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.OBSERVED_VALUES,\n",
    "            ),\n",
    "            # step 4: add temporal features based on freq of the dataset\n",
    "            # month of year in the case when freq=\"M\"\n",
    "            # these serve as positional encodings\n",
    "            AddTimeFeatures(\n",
    "                start_field=FieldName.START,\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                time_features=time_features_from_frequency_str(freq),\n",
    "                pred_length=config.prediction_length,\n",
    "            ),\n",
    "#             # step 5: add another temporal feature (just a single number)\n",
    "#             # tells the model where in the life the value of the time series is\n",
    "#             # sort of running counter\n",
    "#             AddAgeFeature(\n",
    "#                 target_field=FieldName.TARGET,\n",
    "#                 output_field=FieldName.FEAT_AGE,\n",
    "#                 pred_length=config.prediction_length,\n",
    "#                 log_scale=True,\n",
    "#             ),\n",
    "            # step 6: vertically stack all the temporal features into the key FEAT_TIME\n",
    "            VstackFeatures(\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                input_fields=[FieldName.FEAT_TIME]  ## cs: from \"[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\"\n",
    "                + (\n",
    "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
    "                    if config.num_dynamic_real_features > 0\n",
    "                    else []\n",
    "                ),\n",
    "            ),\n",
    "            # step 7: rename to match HuggingFace names\n",
    "            RenameFields(\n",
    "                mapping={\n",
    "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
    "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
    "                    FieldName.FEAT_TIME: \"time_features\",\n",
    "                    FieldName.TARGET: \"values\",\n",
    "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# print(len(FieldName.FEAT_TIME),len(FieldName.FEAT_TIME), len(FieldName.FEAT_DYNAMIC_REAL))\n",
    "for i in [FieldName.FEAT_STATIC_CAT, FieldName.FEAT_STATIC_REAL, FieldName.FEAT_TIME, FieldName.TARGET, FieldName.OBSERVED_VALUES]:\n",
    "    print(i, len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTANCE SPLITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.transform.sampler import InstanceSampler\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def create_instance_splitter(\n",
    "    config: PretrainedConfig,\n",
    "    mode: str,\n",
    "    train_sampler: Optional[InstanceSampler] = None,\n",
    "    validation_sampler: Optional[InstanceSampler] = None,\n",
    ") -> Transformation:\n",
    "    assert mode in [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    instance_sampler = {\n",
    "        \"train\": train_sampler\n",
    "        or ExpectedNumInstanceSampler(\n",
    "            num_instances=1.0, min_future=config.prediction_length\n",
    "        ),\n",
    "        \"validation\": validation_sampler\n",
    "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
    "        \"test\": TestSplitSampler(),\n",
    "    }[mode]\n",
    "\n",
    "    return InstanceSplitter(\n",
    "        target_field=\"values\",\n",
    "        is_pad_field=FieldName.IS_PAD,\n",
    "        start_field=FieldName.START,\n",
    "        forecast_start_field=FieldName.FORECAST_START,\n",
    "        instance_sampler=instance_sampler,\n",
    "        past_length=config.context_length + max(config.lags_sequence),\n",
    "        future_length=config.prediction_length,\n",
    "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning\n",
      "  Obtaining dependency information for pytorch_lightning from https://files.pythonhosted.org/packages/fa/c7/18aca7e74b6c4bb99ceb76a7742716543f040834b8440acad4afaf528e46/pytorch_lightning-2.0.9-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_lightning-2.0.9-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (1.21.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (1.13.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (2023.6.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
      "  Obtaining dependency information for torchmetrics>=0.7.0 from https://files.pythonhosted.org/packages/a3/88/cc27059747ddecff744826e38014822023cbfff4ca079a6ee9a96602dd0b/torchmetrics-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading torchmetrics-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging>=17.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch_lightning) (4.4.0)\n",
      "Collecting lightning-utilities>=0.7.0 (from pytorch_lightning)\n",
      "  Obtaining dependency information for lightning-utilities>=0.7.0 from https://files.pythonhosted.org/packages/46/ee/8641eeb6a062f383b7d6875604e1f3f83bd2c93a0b4dbcabd3150b32de6e/lightning_utilities-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading lightning_utilities-0.9.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=17.1->pytorch_lightning) (3.0.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/franco/Library/Python/3.10/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
      "Downloading pytorch_lightning-2.0.9-py3-none-any.whl (727 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.7/727.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
      "Successfully installed lightning-utilities-0.9.0 pytorch_lightning-2.0.9 torchmetrics-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'IterableDataset' from 'gluonts.torch.util' (/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gluonts/torch/util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco Model Test/ts_test.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgluonts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mitertools\u001b[39;00m \u001b[39mimport\u001b[39;00m Cyclic, IterableSlice, PseudoShuffled\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgluonts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m IterableDataset\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/franco/Desktop/College/Thesis/VHS-Thesis/Franco%20Model%20Test/ts_test.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Iterable\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'IterableDataset' from 'gluonts.torch.util' (/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gluonts/torch/util.py)"
     ]
    }
   ],
   "source": [
    "from gluonts.itertools import Cyclic, IterableSlice, PseudoShuffled\n",
    "from gluonts.torch.util import IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def create_train_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    num_batches_per_epoch: int,\n",
    "    shuffle_buffer_length: Optional[int] = None,\n",
    "    **kwargs,\n",
    ") -> Iterable:\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
    "        \"future_values\",\n",
    "        \"future_observed_mask\",\n",
    "    ]\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=True)\n",
    "\n",
    "    # we initialize a Training instance\n",
    "    instance_splitter = create_instance_splitter(config, \"train\") + SelectFields(\n",
    "        TRAINING_INPUT_NAMES\n",
    "    )\n",
    "\n",
    "    # the instance splitter will sample a window of\n",
    "    # context length + lags + prediction length (from the 366 possible transformed time series)\n",
    "    # randomly from within the target time series and return an iterator.\n",
    "    training_instances = instance_splitter.apply(\n",
    "        Cyclic(transformed_data)\n",
    "        if shuffle_buffer_length is None\n",
    "        else PseudoShuffled(\n",
    "            Cyclic(transformed_data),\n",
    "            shuffle_buffer_length=shuffle_buffer_length,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # from the training instances iterator we now return a Dataloader which will\n",
    "    # continue to sample random windows for as long as it is called\n",
    "    # to return batch_size of the appropriate tensors ready for training!\n",
    "    return IterableSlice(\n",
    "        iter(\n",
    "            DataLoader(\n",
    "                IterableDataset(training_instances),\n",
    "                batch_size=batch_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "        ),\n",
    "        num_batches_per_epoch,\n",
    "    )\n",
    "\n",
    "def create_test_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=False)\n",
    "\n",
    "    # we create a Test Instance splitter which will sample the very last\n",
    "    # context window seen during training only for the encoder.\n",
    "    instance_sampler = create_instance_splitter(config, \"test\") + SelectFields(\n",
    "        PREDICTION_INPUT_NAMES\n",
    "    )\n",
    "\n",
    "    # we apply the transformations in test mode\n",
    "    testing_instances = instance_sampler.apply(transformed_data, is_train=False)\n",
    "\n",
    "    # This returns a Dataloader which will go over the dataset once.\n",
    "    return DataLoader(\n",
    "        IterableDataset(testing_instances), batch_size=batch_size, **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating Dataloaders\n",
    "batch_size = 256\n",
    "\n",
    "lat_train_dataloader = create_train_dataloader(\n",
    "    config=lat_config,\n",
    "    freq=freq,\n",
    "    data=lat_dataset['train'],\n",
    "    batch_size=batch_size,\n",
    "    num_batches_per_epoch=100,\n",
    ")\n",
    "\n",
    "long_train_dataloader = create_train_dataloader(\n",
    "    config=long_config,\n",
    "    freq=freq,\n",
    "    data=long_dataset['train'],\n",
    "    batch_size=batch_size,\n",
    "    num_batches_per_epoch=100,\n",
    ")\n",
    "\n",
    "lat_test_dataloader = create_test_dataloader(\n",
    "    config=lat_config,\n",
    "    freq=freq,\n",
    "    data=lat_dataset['test'],\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "long_test_dataloader = create_test_dataloader(\n",
    "    config=long_config,\n",
    "    freq=freq,\n",
    "    data=long_dataset['test'],\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") ## cs\n",
    "print(device)\n",
    "\n",
    "lat_batch = next(iter(lat_train_dataloader))\n",
    "long_batch = next(iter(long_train_dataloader))\n",
    "\n",
    "# perform forward pass\n",
    "lat_outputs = lat_model(\n",
    "    past_values=lat_batch[\"past_values\"],\n",
    "    past_time_features=lat_batch[\"past_time_features\"],\n",
    "    past_observed_mask=lat_batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=lat_batch[\"static_categorical_features\"]\n",
    "    if lat_config.num_static_categorical_features > 0\n",
    "    else None,\n",
    "    static_real_features=lat_batch[\"static_real_features\"]\n",
    "    if lat_config.num_static_real_features > 0\n",
    "    else torch.zeros((batch_size, 0), dtype=torch.float32, device=device),  ## cs: changed from \"else None\"\n",
    "    future_values=lat_batch[\"future_values\"],\n",
    "    future_time_features=lat_batch[\"future_time_features\"],\n",
    "    future_observed_mask=lat_batch[\"future_observed_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "long_outputs = long_model(\n",
    "    past_values=long_batch[\"past_values\"],\n",
    "    past_time_features=long_batch[\"past_time_features\"],\n",
    "    past_observed_mask=long_batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=long_batch[\"static_categorical_features\"]\n",
    "    if long_config.num_static_categorical_features > 0\n",
    "    else None,\n",
    "    static_real_features=long_batch[\"static_real_features\"]\n",
    "    if long_config.num_static_real_features > 0\n",
    "    else torch.zeros((batch_size, 0), dtype=torch.float32, device=device),  ## cs: changed from \"else None\"\n",
    "    future_values=long_batch[\"future_values\"],\n",
    "    future_time_features=long_batch[\"future_time_features\"],\n",
    "    future_observed_mask=long_batch[\"future_observed_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "\n",
    "lat_accelerator = Accelerator()\n",
    "lat_device = lat_accelerator.device\n",
    "lat_device = \"cpu\"  ## cs\n",
    "\n",
    "lat_model.to(lat_device)\n",
    "long_optimizer = AdamW(lat_model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\n",
    "\n",
    "# model, optimizer, train_dataloader = accelerator.prepare(\n",
    "#     model,\n",
    "#     optimizer,\n",
    "#     train_dataloader,\n",
    "# )\n",
    "\n",
    "lat_model.train()\n",
    "for epoch in range(40):\n",
    "    for idx, batch in enumerate(lat_train_dataloader):\n",
    "        long_optimizer.zero_grad()\n",
    "        outputs = lat_model(\n",
    "            static_categorical_features=batch[\"static_categorical_features\"].to(lat_device)\n",
    "            if lat_config.num_static_categorical_features > 0\n",
    "            else None,\n",
    "            static_real_features=batch[\"static_real_features\"].to(lat_device)\n",
    "            if lat_config.num_static_real_features > 0\n",
    "            else torch.zeros((batch_size, 0), dtype=torch.float32, device=lat_device).to(lat_device),  ## cs: changed from \"else None\"\n",
    "            past_time_features=batch[\"past_time_features\"].to(lat_device),\n",
    "            past_values=batch[\"past_values\"].to(lat_device),\n",
    "            future_time_features=batch[\"future_time_features\"].to(lat_device),\n",
    "            future_values=batch[\"future_values\"].to(lat_device),\n",
    "            past_observed_mask=batch[\"past_observed_mask\"].to(lat_device),\n",
    "            future_observed_mask=batch[\"future_observed_mask\"].to(lat_device),\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        lat_accelerator.backward(loss)\n",
    "        long_optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitude Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "\n",
    "long_accelerator = Accelerator()\n",
    "long_device = long_accelerator.device\n",
    "long_device = \"cpu\"  ## cs\n",
    "\n",
    "long_model.to(device)\n",
    "long_optimizer = AdamW(long_model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\n",
    "\n",
    "# model, optimizer, train_dataloader = accelerator.prepare(\n",
    "#     model,\n",
    "#     optimizer,\n",
    "#     train_dataloader,\n",
    "# )\n",
    "\n",
    "long_model.train()\n",
    "for epoch in range(40):\n",
    "    for idx, batch in enumerate(long_train_dataloader):\n",
    "        long_optimizer.zero_grad()\n",
    "        outputs = long_model(\n",
    "            static_categorical_features=batch[\"static_categorical_features\"].to(long_device)\n",
    "            if long_config.num_static_categorical_features > 0\n",
    "            else None,\n",
    "            static_real_features=batch[\"static_real_features\"].to(long_device)\n",
    "            if long_config.num_static_real_features > 0\n",
    "            else torch.zeros((batch_size, 0), dtype=torch.float32, device=long_device).to(long_device),  ## cs: changed from \"else None\"\n",
    "            past_time_features=batch[\"past_time_features\"].to(long_device),\n",
    "            past_values=batch[\"past_values\"].to(long_device),\n",
    "            future_time_features=batch[\"future_time_features\"].to(long_device),\n",
    "            future_values=batch[\"future_values\"].to(long_device),\n",
    "            past_observed_mask=batch[\"past_observed_mask\"].to(long_device),\n",
    "            future_observed_mask=batch[\"future_observed_mask\"].to(long_device),\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        long_accelerator.backward(loss)\n",
    "        long_optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_model.eval()\n",
    "\n",
    "lat_forecasts = []\n",
    "\n",
    "for batch in lat_test_dataloader:\n",
    "    outputs = lat_model.generate(\n",
    "        static_categorical_features=batch[\"static_categorical_features\"].to(lat_device)\n",
    "        if lat_config.num_static_categorical_features > 0\n",
    "        else None,\n",
    "        static_real_features=batch[\"static_real_features\"].to(lat_device)\n",
    "        if lat_config.num_static_real_features > 0\n",
    "        else torch.zeros((3, 0), dtype=torch.float32, device=lat_device).to(lat_device),  ## cs: changed from \"else None\"\n",
    "        past_time_features=batch[\"past_time_features\"].to(lat_device),\n",
    "        past_values=batch[\"past_values\"].to(lat_device),\n",
    "        future_time_features=batch[\"future_time_features\"].to(lat_device),\n",
    "        past_observed_mask=batch[\"past_observed_mask\"].to(lat_device),\n",
    "    )\n",
    "    lat_forecasts.append(outputs.sequences.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/franco/Library/Python/3.10/lib/python/site-packages (from scikit-learn) (1.9.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lat_forecasts = np.vstack(lat_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "lat_mase_metric = load(\"evaluate-metric/mase\")\n",
    "lat_smape_metric = load(\"evaluate-metric/smape\")\n",
    "lat_mse_metric = load(\"evaluate-metric/mse\")\n",
    "\n",
    "lat_forecast_median = np.median(lat_forecasts, 1)\n",
    "\n",
    "lat_mase_metrics = []\n",
    "lat_smape_metrics = []\n",
    "lat_mse_metrics = []\n",
    "for item_id, ts in enumerate(lat_dataset['test']):\n",
    "    training_data = ts[\"target\"][:-prediction_length]\n",
    "    ground_truth = ts[\"target\"][-prediction_length:]\n",
    "#     mase = mase_metric.compute(   cs: commented out cuz not working\n",
    "#         predictions=forecast_median[item_id],\n",
    "#         references=np.array(ground_truth),\n",
    "#         training=np.array(training_data),\n",
    "#         periodicity=get_seasonality(freq),\n",
    "#     )\n",
    "          \n",
    "#     mase_metrics.append(mase[\"mase\"])\n",
    "    print(lat_forecast_median[item_id])\n",
    "    print(np.array(ground_truth))\n",
    "\n",
    "    long_smape = lat_smape_metric.compute(\n",
    "        predictions=lat_forecast_median[item_id],\n",
    "        references=np.array(ground_truth),\n",
    "    )\n",
    "    lat_smape_metrics.append(long_smape[\"smape\"])\n",
    "    \n",
    "    lat_mse = lat_mse_metric.compute(predictions=lat_forecast_median[item_id], references=np.array(ground_truth))  ## cs: added for mse\n",
    "    lat_mse_metrics.append(lat_mse[\"mse\"])\n",
    "\n",
    "print(f\"sMAPE: {np.mean(lat_smape_metrics)}, MSE: {np.mean(lat_mse_metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitude Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_model.eval()\n",
    "\n",
    "long_forecasts = []\n",
    "\n",
    "for batch in long_test_dataloader:\n",
    "    outputs = long_model.generate(\n",
    "        static_categorical_features=batch[\"static_categorical_features\"].to(long_device)\n",
    "        if long_config.num_static_categorical_features > 0\n",
    "        else None,\n",
    "        static_real_features=batch[\"static_real_features\"].to(long_device)\n",
    "        if long_config.num_static_real_features > 0\n",
    "        else torch.zeros((3, 0), dtype=torch.float32, device=long_device).to(long_device),  ## cs: changed from \"else None\"\n",
    "        past_time_features=batch[\"past_time_features\"].to(long_device),\n",
    "        past_values=batch[\"past_values\"].to(long_device),\n",
    "        future_time_features=batch[\"future_time_features\"].to(long_device),\n",
    "        past_observed_mask=batch[\"past_observed_mask\"].to(long_device),\n",
    "    )\n",
    "    long_forecasts.append(outputs.sequences.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "long_forecasts = np.vstack(long_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "long_mase_metric = load(\"evaluate-metric/mase\")\n",
    "long_smape_metric = load(\"evaluate-metric/smape\")\n",
    "long_mse_metric = load(\"evaluate-metric/mse\")\n",
    "\n",
    "long_forecast_median = np.median(lat_forecasts, 1)\n",
    "\n",
    "long_mase_metrics = []\n",
    "long_smape_metrics = []\n",
    "long_mse_metrics = []\n",
    "for item_id, ts in enumerate(long_dataset['test']):\n",
    "    training_data = ts[\"target\"][:-prediction_length]\n",
    "    ground_truth = ts[\"target\"][-prediction_length:]\n",
    "#     mase = mase_metric.compute(   cs: commented out cuz not working\n",
    "#         predictions=forecast_median[item_id],\n",
    "#         references=np.array(ground_truth),\n",
    "#         training=np.array(training_data),\n",
    "#         periodicity=get_seasonality(freq),\n",
    "#     )\n",
    "          \n",
    "#     mase_metrics.append(mase[\"mase\"])\n",
    "    print(long_forecast_median[item_id])\n",
    "    print(np.array(ground_truth))\n",
    "\n",
    "    long_smape = lat_smape_metric.compute(\n",
    "        predictions=lat_forecast_median[item_id],\n",
    "        references=np.array(ground_truth),\n",
    "    )\n",
    "    lat_smape_metrics.append(long_smape[\"smape\"])\n",
    "    \n",
    "    long_mse = long_mse_metric.compute(predictions=long_forecast_median[item_id], references=np.array(ground_truth))  ## cs: added for mse\n",
    "    long_mse_metrics.append(long_mse[\"mse\"])\n",
    "\n",
    "print(f\"sMAPE: {np.mean(long_smape_metrics)}, MSE: {np.mean(long_mse_metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "def plot(ts_index):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    index = pd.period_range(\n",
    "        start=test_dataset[ts_index][FieldName.START],\n",
    "        periods=len(test_dataset[ts_index][FieldName.TARGET]),\n",
    "        freq=freq,\n",
    "    ).to_timestamp()\n",
    "\n",
    "    # Major ticks every half year, minor ticks every month,\n",
    "#     ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 7)))  ## cs: removed month axis ticks to default as auto\n",
    "#     ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "\n",
    "\n",
    "#     ax.plot(  ## cs: replaced \"-2*prediction_length\" by \"0\" to show full time graph\n",
    "#         index[-2 * prediction_length :],\n",
    "#         test_dataset[ts_index][\"target\"][-2 * prediction_length :],\n",
    "#         label=\"actual\",\n",
    "#     )\n",
    "    ax.plot(\n",
    "        index[0 :],\n",
    "        test_dataset[ts_index][\"target\"][0 :],\n",
    "        label=\"actual\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        index[-prediction_length:],\n",
    "        np.median(forecasts[ts_index], axis=0),\n",
    "        label=\"median\",\n",
    "    )\n",
    "\n",
    "    plt.fill_between(\n",
    "        index[-prediction_length:],\n",
    "        forecasts[ts_index].mean(0) - forecasts[ts_index].std(axis=0),\n",
    "        forecasts[ts_index].mean(0) + forecasts[ts_index].std(axis=0),\n",
    "        alpha=0.3,\n",
    "        interpolate=True,\n",
    "        label=\"+/- 1-std\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for i in range(3):\n",
    "    plot(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
