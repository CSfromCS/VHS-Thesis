{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "allWIe5kwPcS"
   },
   "source": [
    "# Time Series Datasets\n",
    "\n",
    "This notebook shows how to create a time series dataset from some csv file in order to then share it on the [ðŸ¤— hub](https://huggingface.co/docs/datasets/index). We will use the GluonTS library to read the csv into the appropriate format. We start by installing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4XnNcdWbwrNo"
   },
   "outputs": [],
   "source": [
    "! pip install -q datasets gluonts orjson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI1yo_vHw5CV"
   },
   "source": [
    "GluonTS comes with a pandas DataFrame based dataset so our strategy will be to read the csv file, and process it as a `PandasDataset`. We will then iterate over it and convert it to a ðŸ¤— dataset with the appropriate schema for time series. So lets get started!\n",
    "\n",
    "## `PandasDataset`\n",
    "\n",
    "Suppose we are given multiple (10) time series stacked on top of each other in a dataframe with an `item_id` column that distinguishes different series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "e9FaT_VpwuI2",
    "outputId": "8a10c908-41e1-4ca7-b420-01c0810c5c4b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-05-08 08:00:00</th>\n",
       "      <td>14.801429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-08 08:02:00</th>\n",
       "      <td>14.838326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-08 08:04:00</th>\n",
       "      <td>14.999357</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-08 08:06:00</th>\n",
       "      <td>14.985867</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-08 08:08:00</th>\n",
       "      <td>14.966920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        target  item_id\n",
       "time                                   \n",
       "2023-05-08 08:00:00  14.801429        1\n",
       "2023-05-08 08:02:00  14.838326        1\n",
       "2023-05-08 08:04:00  14.999357        1\n",
       "2023-05-08 08:06:00  14.985867        1\n",
       "2023-05-08 08:08:00  14.966920        1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = (\"datatocsv.csv\"\n",
    ")\n",
    "df = pd.read_csv(url, index_col=0, parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4WCjvBqxi3B"
   },
   "source": [
    "After converting it into a `pd.Dataframe` we can then convert it into GluonTS's `PandasDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "su1i8ZdDxf7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PandasDataset(target='target', feat_dynamic_real=None, past_feat_dynamic_real=None, timestamp=None, freq='2T', future_length=0, unchecked=False, assume_sorted=False, dtype=<class 'numpy.float32'>, _data_entries=StarMap(fn=<bound method PandasDataset._pair_to_dataentry of ...>, iterable=Map(fn=<function pair_with_item_id at 0x11e144820>, iterable=<pandas.core.groupby.generic.DataFrameGroupBy object at 0x11e0ecbe0>)), _static_reals=Empty DataFrame\n",
       "Columns: []\n",
       "Index: [], _static_cats=Empty DataFrame\n",
       "Columns: []\n",
       "Index: [])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gluonts.dataset.pandas import PandasDataset\n",
    "\n",
    "ds = PandasDataset.from_long_dataframe(df, target=\"target\", item_id=\"item_id\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYnHkLdex_n3"
   },
   "source": [
    "\n",
    "## ðŸ¤— Datasets\n",
    "\n",
    "From here we have to map the pandas dataset's `start` field into a time stamp instead of a `pd.Period`. We do this by defining the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gv5Ytpwrx4FQ"
   },
   "outputs": [],
   "source": [
    "class ProcessStartField():\n",
    "    ts_id = 0\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        data[\"start\"] = data[\"start\"].to_timestamp()\n",
    "        data[\"feat_static_cat\"] = [self.ts_id]\n",
    "        self.ts_id += 1\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DrhbT1QIyPMA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': Timestamp('2023-05-08 08:00:00'),\n",
       "  'target': array([14.80142859, 14.83832621, 14.99935666, 14.98586702, 14.96691959,\n",
       "         14.76594389, 14.40506606, 14.24233518, 14.1078629 , 14.02005897,\n",
       "         14.02686409, 14.11167619, 14.37431213, 14.81463118, 14.87351203,\n",
       "         14.98560086, 14.89712766, 14.72523137, 14.45725261, 14.1182195 ,\n",
       "         14.04149182, 14.01007506, 14.10148797, 14.19708527, 14.63541159,\n",
       "         14.65095132, 14.92064957, 14.95902401, 14.92891783, 14.66951365,\n",
       "         14.22168273, 14.00022455, 14.01492854, 14.18395849, 14.20266031,\n",
       "         14.51307107, 14.59690551, 14.7338889 , 14.94700962, 14.97363846,\n",
       "         14.91797743, 14.62208994, 14.35220207, 14.13212672, 14.01095832,\n",
       "         14.26102337, 14.40147358, 14.43267948, 14.838395  , 14.87263943,\n",
       "         14.94588983, 14.99249156, 14.89858088, 14.50215934, 14.37503622,\n",
       "         14.03416362, 14.02092854, 14.00505701, 14.04673264, 14.09335009,\n",
       "         14.33901749, 14.74294692, 14.99656327, 14.83160007, 14.68694722,\n",
       "         14.2637757 ]),\n",
       "  'item_id': 1,\n",
       "  'feat_static_cat': [0]},\n",
       " {'start': Timestamp('2023-05-08 02:10:00'),\n",
       "  'target': array([14.55627681, 14.91260622, 14.94160073, 14.99013056, 14.78851187,\n",
       "         14.32030025, 14.14089156, 14.00100718, 14.01119265, 14.2851912 ,\n",
       "         14.53234866, 14.68640706, 14.85908114, 14.99813384, 14.91977187,\n",
       "         14.7284205 , 14.58777572, 14.54707597, 14.13473958, 14.06193255,\n",
       "         14.00010652, 14.03325   , 14.22883725, 14.25715075, 14.62369196,\n",
       "         14.73462985, 14.99565841, 14.84892723, 14.71709997, 14.41996734,\n",
       "         14.31416611, 14.2334485 , 14.1095664 , 14.00000295, 14.19186388,\n",
       "         14.29331988, 14.39147053, 14.41014489, 14.75592397, 14.87546171,\n",
       "         14.99282209, 14.75300551, 14.5745062 , 14.44741398, 14.08347599,\n",
       "         14.00055368, 14.00004399, 14.00826727, 14.09958466, 14.25717064,\n",
       "         14.48433789, 14.90975623, 14.98384873, 14.99518632, 14.74501139]),\n",
       "  'item_id': 2,\n",
       "  'feat_static_cat': [1]},\n",
       " {'start': Timestamp('2023-05-09 08:05:00'),\n",
       "  'target': array([14.76268179, 14.98358081, 14.98984188, 14.82172956, 14.42450142,\n",
       "         14.06841364, 14.00921465, 14.13289254, 14.48683392, 14.51165244,\n",
       "         14.77547969, 14.91400659, 14.94653905, 14.93150341, 14.71183126,\n",
       "         14.39098422, 14.2233393 , 14.07615557, 14.01260928, 14.22170487,\n",
       "         14.27256225, 14.33088254, 14.43772187, 14.57566187, 14.86244706,\n",
       "         14.93424676, 14.96491821, 14.96884436, 14.72506522, 14.44867555,\n",
       "         14.27208723, 14.1322359 , 14.01586445, 14.00913863, 14.24073421,\n",
       "         14.36863431, 14.5491803 , 14.89708895, 14.9928827 , 14.99440496,\n",
       "         14.96112757, 14.93660813, 14.64768483, 14.37936579, 14.20027479,\n",
       "         14.00410415, 14.02697149, 14.05812018, 14.38136197, 14.82283952,\n",
       "         14.94743409, 14.99983686, 14.92977892, 14.70837462, 14.24987723,\n",
       "         14.21938573, 14.14058053, 14.0044973 , 14.04689642, 14.36986365,\n",
       "         14.76400595, 14.88879484, 14.9581624 , 14.98467766, 14.96088268,\n",
       "         14.8495938 , 14.52575263, 14.44864457, 14.12826099, 14.00480968,\n",
       "         14.01949197, 14.2348848 , 14.65467287, 14.66017259, 14.97396911,\n",
       "         14.99996745, 14.78114888, 14.46689416, 14.40956815, 14.32576679,\n",
       "         14.13354281, 14.08296187, 14.00006649, 14.05714148, 14.42670588,\n",
       "         14.8267862 , 14.92542211, 14.999357  , 14.99869218]),\n",
       "  'item_id': 3,\n",
       "  'feat_static_cat': [2]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gluonts.itertools import Map\n",
    "\n",
    "process_start = ProcessStartField()\n",
    "\n",
    "list_ds = list(Map(process_start, ds))\n",
    "list_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ug2kLNUPyeyJ"
   },
   "source": [
    "Next we need to define our schema features and create our dataset from this list via the `from_list` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r1rQUtvGycaF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Features, Value, Sequence\n",
    "\n",
    "features  = Features(\n",
    "    {    \n",
    "        \"start\": Value(\"timestamp[s]\"),\n",
    "        \"target\": Sequence(Value(\"float32\")),\n",
    "        \"feat_static_cat\": Sequence(Value(\"uint64\")),\n",
    "        # \"feat_static_real\":  Sequence(Value(\"float32\")),\n",
    "        # \"feat_dynamic_real\": Sequence(Sequence(Value(\"uint64\"))),\n",
    "        # \"feat_dynamic_cat\": Sequence(Sequence(Value(\"uint64\"))),\n",
    "        \"item_id\": Value(\"string\"),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RrpP2oAxywC8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': datetime.datetime(2023, 5, 8, 8, 0),\n",
       " 'target': [14.80142879486084,\n",
       "  14.838326454162598,\n",
       "  14.999356269836426,\n",
       "  14.98586654663086,\n",
       "  14.966919898986816,\n",
       "  14.76594352722168,\n",
       "  14.40506649017334,\n",
       "  14.242335319519043,\n",
       "  14.10786247253418,\n",
       "  14.020058631896973,\n",
       "  14.026864051818848,\n",
       "  14.111676216125488,\n",
       "  14.374312400817871,\n",
       "  14.814631462097168,\n",
       "  14.873512268066406,\n",
       "  14.985600471496582,\n",
       "  14.897128105163574,\n",
       "  14.725231170654297,\n",
       "  14.457252502441406,\n",
       "  14.118219375610352,\n",
       "  14.041491508483887,\n",
       "  14.010074615478516,\n",
       "  14.10148811340332,\n",
       "  14.1970853805542,\n",
       "  14.635411262512207,\n",
       "  14.650951385498047,\n",
       "  14.920649528503418,\n",
       "  14.959024429321289,\n",
       "  14.92891788482666,\n",
       "  14.669513702392578,\n",
       "  14.22168254852295,\n",
       "  14.000224113464355,\n",
       "  14.014928817749023,\n",
       "  14.183958053588867,\n",
       "  14.20266056060791,\n",
       "  14.513071060180664,\n",
       "  14.596905708312988,\n",
       "  14.733888626098633,\n",
       "  14.947010040283203,\n",
       "  14.973638534545898,\n",
       "  14.917977333068848,\n",
       "  14.622090339660645,\n",
       "  14.352202415466309,\n",
       "  14.132126808166504,\n",
       "  14.010958671569824,\n",
       "  14.26102352142334,\n",
       "  14.401473999023438,\n",
       "  14.432679176330566,\n",
       "  14.838395118713379,\n",
       "  14.872639656066895,\n",
       "  14.945889472961426,\n",
       "  14.992491722106934,\n",
       "  14.898580551147461,\n",
       "  14.502159118652344,\n",
       "  14.375036239624023,\n",
       "  14.034163475036621,\n",
       "  14.020928382873535,\n",
       "  14.005057334899902,\n",
       "  14.046732902526855,\n",
       "  14.093350410461426,\n",
       "  14.339017868041992,\n",
       "  14.74294662475586,\n",
       "  14.996562957763672,\n",
       "  14.831600189208984,\n",
       "  14.686946868896484,\n",
       "  14.263775825500488],\n",
       " 'item_id': '1',\n",
       " 'feat_static_cat': [0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_list(list_ds, features=features)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making our own dummy ejeep dataset\n",
    "from https://docs.google.com/spreadsheets/d/1pLvLAbwpJNh3cB95i1ZhIj4Wu7qeUOR1og9IoQK7XjI/edit#gid=1255010272\n",
    "dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.80142859 14.83832621 14.99935666 14.98586702 14.96691959 14.76594389\n",
      " 14.40506606 14.24233518 14.1078629  14.02005897 14.02686409 14.11167619\n",
      " 14.37431213 14.81463118 14.87351203 14.98560086 14.89712766 14.72523137\n",
      " 14.45725261 14.1182195  14.04149182 14.01007506 14.10148797 14.19708527\n",
      " 14.63541159 14.65095132 14.92064957 14.95902401 14.92891783 14.66951365\n",
      " 14.22168273 14.00022455 14.01492854 14.18395849 14.20266031 14.51307107\n",
      " 14.59690551 14.7338889  14.94700962 14.97363846 14.91797743 14.62208994\n",
      " 14.35220207 14.13212672 14.01095832 14.26102337 14.40147358 14.43267948\n",
      " 14.838395   14.87263943 14.94588983]\n"
     ]
    }
   ],
   "source": [
    "from gluonts.itertools import Map\n",
    "\n",
    "process_start = ProcessStartField()\n",
    "\n",
    "list_ds_train = list(Map(process_start, ds))\n",
    "for dst in list_ds_train:\n",
    "    dst['target'] = dst['target'][:-15]\n",
    "\n",
    "print(list_ds_train[0]['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': datetime.datetime(2023, 5, 8, 8, 0),\n",
       " 'target': [14.80142879486084,\n",
       "  14.838326454162598,\n",
       "  14.999356269836426,\n",
       "  14.98586654663086,\n",
       "  14.966919898986816,\n",
       "  14.76594352722168,\n",
       "  14.40506649017334,\n",
       "  14.242335319519043,\n",
       "  14.10786247253418,\n",
       "  14.020058631896973,\n",
       "  14.026864051818848,\n",
       "  14.111676216125488,\n",
       "  14.374312400817871,\n",
       "  14.814631462097168,\n",
       "  14.873512268066406,\n",
       "  14.985600471496582,\n",
       "  14.897128105163574,\n",
       "  14.725231170654297,\n",
       "  14.457252502441406,\n",
       "  14.118219375610352,\n",
       "  14.041491508483887,\n",
       "  14.010074615478516,\n",
       "  14.10148811340332,\n",
       "  14.1970853805542,\n",
       "  14.635411262512207,\n",
       "  14.650951385498047,\n",
       "  14.920649528503418,\n",
       "  14.959024429321289,\n",
       "  14.92891788482666,\n",
       "  14.669513702392578,\n",
       "  14.22168254852295,\n",
       "  14.000224113464355,\n",
       "  14.014928817749023,\n",
       "  14.183958053588867,\n",
       "  14.20266056060791,\n",
       "  14.513071060180664,\n",
       "  14.596905708312988,\n",
       "  14.733888626098633,\n",
       "  14.947010040283203,\n",
       "  14.973638534545898,\n",
       "  14.917977333068848,\n",
       "  14.622090339660645,\n",
       "  14.352202415466309,\n",
       "  14.132126808166504,\n",
       "  14.010958671569824,\n",
       "  14.26102352142334,\n",
       "  14.401473999023438,\n",
       "  14.432679176330566,\n",
       "  14.838395118713379,\n",
       "  14.872639656066895,\n",
       "  14.945889472961426],\n",
       " 'item_id': '1',\n",
       " 'feat_static_cat': [0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = Dataset.from_list(list_ds_train, features=features)\n",
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[2]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train[2]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['start', 'target', 'item_id', 'feat_static_cat'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset_train\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['start', 'target', 'item_id', 'feat_static_cat'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"train\": train_dataset, \"test\":test_dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update start to pd.period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9-a3wNZ9mxWT",
   "metadata": {
    "id": "9-a3wNZ9mxWT"
   },
   "source": [
    "# Probabilistic Time Series Forecasting with ðŸ¤— Transformers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Time series forecasting is an essential scientific and business problem and as such has also seen a lot of innovation recently with the use of [deep learning based](https://dl.acm.org/doi/abs/10.1145/3533382) models in addition to the [classical methods](https://otexts.com/fpp3/). An important difference between classical methods like ARIMA and novel deep learning methods is the following.\n",
    "\n",
    "##  Probabilistic Forecasting\n",
    "\n",
    "Typically, classical methods are fitted on each time series in a dataset individually. These are often referred to as  \"single\" or \"local\" methods. However, when dealing with a large amount of time series for some applications, it is beneficial to train a \"global\" model on all available time series, which enables the model to learn latent representations from many different sources.\n",
    "\n",
    "Some classical methods are point-valued (meaning, they just output a single value per time step) and models are trained by minimizing an L2 or L1 type of loss with respect to the ground truth data. However, since forecasts are often used in some real-world decision making pipeline, even with humans in the loop, it is much more beneficial to provide the uncertainties of predictions. This is also called \"probabilistic forecasting\", as opposed to \"point forecasting\". This entails modeling a probabilistic distribution, from which one can sample.\n",
    "\n",
    "So in short, rather than training local point forecasting models, we hope to train **global probabilistic** models. Deep learning is a great fit for this, as neural networks can learn representations from several related time series as well as model the uncertainty of the data.\n",
    "\n",
    "It is common in the probabilistic setting to learn the future parameters of some chosen parametric distribution, like Gaussian or Student-T; or learn the conditional quantile function; or use the framework of Conformal Prediction adapted to the time series setting. The choice of method does not affect the modeling aspect and thus can be typically thought of as yet another hyperparameter. One can always turn a probabilistic model into a point-forecasting model, by taking empirical means or medians.\n",
    "\n",
    "## The Time Series Transformer\n",
    "\n",
    "In terms of modeling time series data which are sequential in nature, as one can imagine, researchers have come up with models which use Recurrent Neural Networks (RNN) like LSTM or GRU, or Convolutional Networks (CNN), and more recently Transformer based methods which fit naturally to the time series forecasting setting.\n",
    "\n",
    "In this blog post, we're going to leverage the vanilla Transformer [(Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) for the **univariate** probabilistic forecasting task (i.e. predicting each time series' 1-d distribution individually). The Encoder-Decoder Transformer is a natural choice for forecasting as it encapsulates several inductive biases nicely.\n",
    "\n",
    "To begin with, the use of an Encoder-Decoder architecture is helpful at inference time where typically for some logged data we wish to forecast some prediction steps into the future. This can be thought of as analogous to the text generation task where given some context, we sample the next token and pass it back into the decoder (also called \"autoregressive generation\"). Similarly here we can also, given some distribution type, sample from it to provide forecasts up until our desired prediction horizon. This is known as Greedy Sampling/Search and there is a great blog post about it [here](https://huggingface.co/blog/how-to-generate) for the NLP setting.\n",
    "\n",
    "Secondly, a Transformer helps us to train on time series data which might contain thousands of time points. It might not be feasible to input *all* the history of a time series at once to the model, due to the time- and memory constraints of the attention mechanism. Thus, one can consider some appropriate context window and sample this window and the subsequent prediction length sized window from the training data when constructing batches for stochastic gradient descent (SGD). The context sized window can be passed to the encoder and the prediction window to a *causal-masked* decoder. This means that the decoder can only look at previous time steps when learning the next value. This is equivalent to how one would train a vanilla Transformer for machine translation, referred to as \"teacher forcing\".\n",
    "\n",
    "Another benefit of Transformers over the other architectures is that we can incorporate missing values (which are common in the time series setting) as an additional mask to the encoder or decoder and still train without resorting to in-filling or imputation. This is equivalent to the `attention_mask` of models like BERT and GPT-2 in the Transformers library, to not include padding tokens in the computation of the attention matrix.\n",
    "\n",
    "A drawback of the Transformer architecture is the limit to the sizes of the context and prediction windows because of the quadratic compute and memory requirements of the vanilla Transformer, see [Tay et al., 2020](https://arxiv.org/abs/2009.06732). Additionally, since the Transformer is a powerful architecture, it might overfit or learn spurious correlations much more easily compared to other [methods](https://openreview.net/pdf?id=D7YBmfX_VQy).\n",
    "\n",
    "The ðŸ¤— Transformers library comes with a vanilla probabilistic time series Transformer model, simply called the [Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer). In the sections below, we'll show how to train such a model on a custom dataset.\n",
    "\n",
    "## Set-up Environment\n",
    "\n",
    "First, let's install the necessary libraries: ðŸ¤— Transformers, ðŸ¤— Datasets, ðŸ¤— Evaluate,  ðŸ¤— Accelerate and [GluonTS](https://github.com/awslabs/gluonts).\n",
    "\n",
    "As we will show, GluonTS will be used for transforming the data to create features as well as for creating appropriate training, validation and test batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7AkJMJAunLP9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7AkJMJAunLP9",
    "outputId": "b57a63c6-d3da-4034-b081-e3f2da24a8f4"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd9NeR_ZnSWP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd9NeR_ZnSWP",
    "outputId": "647bfe07-8511-4517-ecff-11ef0bf299b7"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "xl9uzDOCKMoK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xl9uzDOCKMoK",
    "outputId": "e88bb8e2-4e4e-4f08-ea52-a90bb8c54faf"
   },
   "outputs": [],
   "source": [
    "!pip install -q evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3X4oByTHPudz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3X4oByTHPudz",
    "outputId": "65c9a838-5716-48d7-b1d6-df946c4c7da9"
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6kDYa76nU9J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6kDYa76nU9J",
    "outputId": "e2d16392-7553-452c-a2a2-3101a18a190d"
   },
   "outputs": [],
   "source": [
    "!pip install -q gluonts ujson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaebea67",
   "metadata": {
    "id": "eaebea67"
   },
   "source": [
    "We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "oHlRCUPkoN1N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHlRCUPkoN1N",
    "outputId": "fe314df1-b6f8-45b9-b3a9-fe7caf44d30b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start', 'target', 'item_id', 'feat_static_cat'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_example = dataset[\"train\"][0]\n",
    "train_example.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gbVF5vAcqzJG",
   "metadata": {
    "id": "gbVF5vAcqzJG"
   },
   "source": [
    "The `start` simply indicates the start of the time series (as a datetime), and the `target` contains the actual values of the time series.\n",
    "\n",
    "The `start` will be useful to add time related features to the time series values, as extra input to the model (such as \"month of year\"). Since we know the frequency of the data is `monthly`, we know for instance that the second value has the timestamp `1979-02-01`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1PDt8bvwoUbN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PDt8bvwoUbN",
    "outputId": "774b6c03-b1c9-4e7a-f3b8-a31a23abdb51",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-08 08:00:00\n",
      "[14.80142879486084, 14.838326454162598, 14.999356269836426, 14.98586654663086, 14.966919898986816, 14.76594352722168, 14.40506649017334, 14.242335319519043, 14.10786247253418, 14.020058631896973, 14.026864051818848, 14.111676216125488, 14.374312400817871, 14.814631462097168, 14.873512268066406, 14.985600471496582, 14.897128105163574, 14.725231170654297, 14.457252502441406, 14.118219375610352, 14.041491508483887, 14.010074615478516, 14.10148811340332, 14.1970853805542, 14.635411262512207, 14.650951385498047, 14.920649528503418, 14.959024429321289, 14.92891788482666, 14.669513702392578, 14.22168254852295, 14.000224113464355, 14.014928817749023, 14.183958053588867, 14.20266056060791, 14.513071060180664, 14.596905708312988, 14.733888626098633, 14.947010040283203, 14.973638534545898, 14.917977333068848, 14.622090339660645, 14.352202415466309, 14.132126808166504, 14.010958671569824, 14.26102352142334, 14.401473999023438, 14.432679176330566, 14.838395118713379, 14.872639656066895, 14.945889472961426]\n"
     ]
    }
   ],
   "source": [
    "print(train_example[\"start\"])\n",
    "print(train_example[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DfkPxhCkquKL",
   "metadata": {
    "id": "DfkPxhCkquKL"
   },
   "source": [
    "The validation set contains the same data as the training set, just for a `prediction_length` longer amount of time. This allows us to validate the model's predictions against the ground truth.\n",
    "\n",
    "The test set is again one `prediction_length` longer data compared to the validation set (or some multiple of  `prediction_length` longer data compared to the training set for testing on multiple rolling windows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eRQhm4EGpa0y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRQhm4EGpa0y",
    "outputId": "bb5e8298-cd62-4cd8-bb54-62377d8f8ca1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start', 'target', 'item_id', 'feat_static_cat'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_example = dataset[\"test\"][0]\n",
    "validation_example.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x5PI_Jn7rDfj",
   "metadata": {
    "id": "x5PI_Jn7rDfj"
   },
   "source": [
    "The initial values are exactly the same as the corresponding training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "yHQnXZbUpg8q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHQnXZbUpg8q",
    "outputId": "8833826d-59e2-4345-ad9d-9b5c9a46b15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-08 08:00:00\n",
      "[14.80142879486084, 14.838326454162598, 14.999356269836426, 14.98586654663086, 14.966919898986816, 14.76594352722168, 14.40506649017334, 14.242335319519043, 14.10786247253418, 14.020058631896973, 14.026864051818848, 14.111676216125488, 14.374312400817871, 14.814631462097168, 14.873512268066406, 14.985600471496582, 14.897128105163574, 14.725231170654297, 14.457252502441406, 14.118219375610352, 14.041491508483887, 14.010074615478516, 14.10148811340332, 14.1970853805542, 14.635411262512207, 14.650951385498047, 14.920649528503418, 14.959024429321289, 14.92891788482666, 14.669513702392578, 14.22168254852295, 14.000224113464355, 14.014928817749023, 14.183958053588867, 14.20266056060791, 14.513071060180664, 14.596905708312988, 14.733888626098633, 14.947010040283203, 14.973638534545898, 14.917977333068848, 14.622090339660645, 14.352202415466309, 14.132126808166504, 14.010958671569824, 14.26102352142334, 14.401473999023438, 14.432679176330566, 14.838395118713379, 14.872639656066895, 14.945889472961426, 14.992491722106934, 14.898580551147461, 14.502159118652344, 14.375036239624023, 14.034163475036621, 14.020928382873535, 14.005057334899902, 14.046732902526855, 14.093350410461426, 14.339017868041992, 14.74294662475586, 14.996562957763672, 14.831600189208984, 14.686946868896484, 14.263775825500488]\n"
     ]
    }
   ],
   "source": [
    "print(validation_example[\"start\"])\n",
    "print(validation_example[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vQU8V7hPrIZA",
   "metadata": {
    "id": "vQU8V7hPrIZA"
   },
   "source": [
    "However, this example has `prediction_length=24` additional values compared to the training example. Let us verify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "__j4Z5Ohp8gg",
   "metadata": {
    "id": "__j4Z5Ohp8gg"
   },
   "outputs": [],
   "source": [
    "freq = \"2M\"\n",
    "prediction_length = 15\n",
    "\n",
    "assert len(train_example[\"target\"]) + prediction_length == len(\n",
    "    validation_example[\"target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PGq2e9D-rhtg",
   "metadata": {
    "id": "PGq2e9D-rhtg"
   },
   "source": [
    "Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cYDyml0tsnlL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "cYDyml0tsnlL",
    "outputId": "0942f1ea-b333-4af4-d0d2-8cd3034b104d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure, axes = plt.subplots()\n",
    "axes.plot(train_example[\"target\"], color=\"blue\")\n",
    "axes.plot(validation_example[\"target\"], color=\"red\", alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4pGV6_ZduUaA",
   "metadata": {
    "id": "4pGV6_ZduUaA"
   },
   "source": [
    "Let's split up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eb15a6a",
   "metadata": {
    "id": "7eb15a6a"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125722c",
   "metadata": {
    "id": "0125722c"
   },
   "source": [
    "## Update `start` to `pd.Period`\n",
    "\n",
    "The first thing we'll do is convert the `start` feature of each time series to a pandas `Period` index using the data's `freq`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "249a9da4",
   "metadata": {
    "id": "249a9da4"
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@lru_cache(10_000)\n",
    "def convert_to_pandas_period(date, freq):\n",
    "    return pd.Period(date, freq)\n",
    "\n",
    "\n",
    "def transform_start_field(batch, freq):\n",
    "    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D7goNkUB1MPB",
   "metadata": {
    "id": "D7goNkUB1MPB"
   },
   "source": [
    "We now use `datasets`' [`set_transform`](https://huggingface.co/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.set_transform) functionality to do this on-the-fly in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b43c7551",
   "metadata": {
    "id": "b43c7551"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "train_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
    "test_dataset.set_transform(partial(transform_start_field, freq=freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efb119",
   "metadata": {
    "id": "50efb119"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "Next, let's instantiate a model. The model will be trained from scratch, hence we won't use the `from_pretrained` method here, but rather randomly initialize the model from a [`config`](https://huggingface.co/docs/transformers/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig).\n",
    "\n",
    "We specify a couple of additional parameters to the model:\n",
    "- `prediction_length` (in our case, `24` months): this is the horizon that the decoder of the Transformer will learn to predict for;\n",
    "- `context_length`: the model will set the `context_length` (input of the encoder) equal to the `prediction_length`, if no `context_length` is specified;\n",
    "- `lags` for a given frequency: these specify how much we \"look back\", to be added as additional features. e.g. for a `Daily` frequency we might consider a look back of `[1, 2, 7, 30, ...]` or in other words look back 1, 2, ... days while for `Minute` data we might consider `[1, 30, 60, 60*24, ...]` etc.;\n",
    "- the number of time features: in our case, this will be `2` as we'll add `MonthOfYear` and `Age` features;\n",
    "- the number of static categorical features: in our case, this will be just `1` as we'll add a single \"time series ID\" feature;\n",
    "- the cardinality: the number of values of each static categorical feature, as a list which for our case will be `[366]` as we have 366 different time series\n",
    "- the embedding dimension: the embedding dimension for each static categorical feature, as a list, for example `[3]` meaning the model will learn an embedding vector of size `3` for each of the `366` time series (regions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0z4YFov8CNUu",
   "metadata": {
    "id": "0z4YFov8CNUu"
   },
   "source": [
    "Let's use the default lags provided by GluonTS for the given frequency (\"monthly\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6CF4M8Ms7W-q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CF4M8Ms7W-q",
    "outputId": "4481a9bd-343c-45bd-b569-88d233ab8d40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "from gluonts.time_feature import get_lags_for_frequency\n",
    "\n",
    "lags_sequence = get_lags_for_frequency(freq)\n",
    "print(lags_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q6ZuFx8yCSAM",
   "metadata": {
    "id": "q6ZuFx8yCSAM"
   },
   "source": [
    "This means that we'll look back up to 37 months for each time step, as additional features.\n",
    "\n",
    "Let's also check the default time features which GluonTS provides us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "VlP_0E5I76lg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VlP_0E5I76lg",
    "outputId": "03d14e28-8609-4c8e-ef7c-efb17f41076a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function month_of_year at 0x11e898ca0>]\n"
     ]
    }
   ],
   "source": [
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "\n",
    "time_features = time_features_from_frequency_str(freq)\n",
    "print(time_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m0_f7lm9CbNG",
   "metadata": {
    "id": "m0_f7lm9CbNG"
   },
   "source": [
    "In this case, there's only a single feature, namely \"month of year\". This means that for each time step, we'll add the month as a scalar value (e.g. `1` in case the timestamp is \"january\", `2` in case the timestamp is \"february\", etc.).\n",
    "\n",
    "We now have everything to define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dda0e78",
   "metadata": {
    "id": "3dda0e78"
   },
   "outputs": [],
   "source": [
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
    "\n",
    "config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=prediction_length,\n",
    "    # context length:\n",
    "    context_length=prediction_length * 2,\n",
    "    # lags coming from helper given the freq:\n",
    "    lags_sequence=lags_sequence,\n",
    "    # we'll add 2 time features (\"month of year\" and \"age\", see further):\n",
    "    num_time_features=len(time_features) + 1,\n",
    "    # we have a single static categorical feature, namely time series ID:\n",
    "    num_static_categorical_features=1,\n",
    "    # it has 366 possible values:\n",
    "    cardinality=[len(train_dataset)],\n",
    "    # the model will learn an embedding of size 2 for each of the 366 possible values:\n",
    "    embedding_dimension=[2],\n",
    "    \n",
    "    # transformer params:\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    d_model=32,\n",
    ")\n",
    "\n",
    "model = TimeSeriesTransformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T6rCeV4dsUnF",
   "metadata": {
    "id": "T6rCeV4dsUnF"
   },
   "source": [
    "Note that, similar to other models in the ðŸ¤— Transformers library, [`TimeSeriesTransformerModel`](https://huggingface.co/docs/transformers/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel) corresponds to the encoder-decoder Transformer without any head on top, and [`TimeSeriesTransformerForPrediction`](https://huggingface.co/docs/transformers/model_doc/time_series_transformer#transformers.TimeSeriesTransformerForPrediction) corresponds to `TimeSeriesTransformerModel` with a **distribution head** on top. By default, the model uses a Student-t distribution (but this is configurable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "EaoKZyujsuIA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "EaoKZyujsuIA",
    "outputId": "e711cbea-c734-463f-f28c-4009e5317a4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'student_t'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.distribution_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feQYVhgus-yl",
   "metadata": {
    "id": "feQYVhgus-yl"
   },
   "source": [
    "This is an important difference with Transformers for NLP, where the head typically consists of a fixed categorical distribution implemented as an `nn.Linear` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82c60d",
   "metadata": {
    "id": "af82c60d"
   },
   "source": [
    "## Define Transformations\n",
    "\n",
    "Next, we define the transformations for the data, in particular for the creation of the time features (based on the dataset or universal ones).\n",
    "\n",
    "Again, we'll use the GluonTS library for this. We define a `Chain` of transformations (which is a bit comparable to `torchvision.transforms.Compose` for images). It allows us to combine several transformations into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "tR87yKPSn8SZ",
   "metadata": {
    "id": "tR87yKPSn8SZ"
   },
   "outputs": [],
   "source": [
    "from gluonts.time_feature import (\n",
    "    time_features_from_frequency_str,\n",
    "    TimeFeature,\n",
    "    get_lags_for_frequency,\n",
    ")\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    AsNumpyArray,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    RemoveFields,\n",
    "    SelectFields,\n",
    "    SetField,\n",
    "    TestSplitSampler,\n",
    "    Transformation,\n",
    "    ValidationSplitSampler,\n",
    "    VstackFeatures,\n",
    "    RenameFields,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4Qql4CInFWO7",
   "metadata": {
    "id": "4Qql4CInFWO7"
   },
   "source": [
    "The transformations below are annotated with comments, to explain what they do. At a high level, we will iterate over the individual time series of our dataset and add/remove fields or features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20fe036e",
   "metadata": {
    "id": "20fe036e"
   },
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
    "    remove_field_names = []\n",
    "    if config.num_static_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
    "    if config.num_dynamic_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
    "    if config.num_static_categorical_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
    "\n",
    "    # a bit like torchvision.transforms.Compose\n",
    "    return Chain(\n",
    "        # step 1: remove static/dynamic fields if not specified\n",
    "        [RemoveFields(field_names=remove_field_names)]\n",
    "        # step 2: convert the data to NumPy (potentially not needed)\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_CAT,\n",
    "                    expected_ndim=1,\n",
    "                    dtype=int,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_REAL,\n",
    "                    expected_ndim=1,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_real_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + [\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.TARGET,\n",
    "                # we expect an extra dim for the multivariate case:\n",
    "                expected_ndim=1 if config.input_size == 1 else 2,\n",
    "            ),\n",
    "            # step 3: handle the NaN's by filling in the target with zero\n",
    "            # and return the mask (which is in the observed values)\n",
    "            # true for observed values, false for nan's\n",
    "            # the decoder uses this mask (no loss is incurred for unobserved values)\n",
    "            # see loss_weights inside the xxxForPrediction model\n",
    "            AddObservedValuesIndicator(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.OBSERVED_VALUES,\n",
    "            ),\n",
    "            # step 4: add temporal features based on freq of the dataset\n",
    "            # month of year in the case when freq=\"M\"\n",
    "            # these serve as positional encodings\n",
    "            AddTimeFeatures(\n",
    "                start_field=FieldName.START,\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                time_features=time_features_from_frequency_str(freq),\n",
    "                pred_length=config.prediction_length,\n",
    "            ),\n",
    "            # step 5: add another temporal feature (just a single number)\n",
    "            # tells the model where in the life the value of the time series is\n",
    "            # sort of running counter\n",
    "            AddAgeFeature(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_AGE,\n",
    "                pred_length=config.prediction_length,\n",
    "                log_scale=True,\n",
    "            ),\n",
    "            # step 6: vertically stack all the temporal features into the key FEAT_TIME\n",
    "            VstackFeatures(\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
    "                + (\n",
    "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
    "                    if config.num_dynamic_real_features > 0\n",
    "                    else []\n",
    "                ),\n",
    "            ),\n",
    "            # step 7: rename to match HuggingFace names\n",
    "            RenameFields(\n",
    "                mapping={\n",
    "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
    "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
    "                    FieldName.FEAT_TIME: \"time_features\",\n",
    "                    FieldName.TARGET: \"values\",\n",
    "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab46d0",
   "metadata": {
    "id": "8bab46d0"
   },
   "source": [
    "## Define `InstanceSplitter`\n",
    "\n",
    "For training/validation/testing we next create an `InstanceSplitter` which is used to sample windows from the dataset (as, remember, we can't pass the entire history of values to the Transformer due to time- and memory constraints).\n",
    "\n",
    "The instance splitter samples random `context_length` sized and subsequent `prediction_length` sized windows from the data, and appends a `past_` or `future_` key to any temporal keys for the respective windows. This makes sure that the `values` will be split into `past_values` and subsequent `future_values` keys, which will serve as the encoder and decoder inputs respectively. The same happens for any keys in the `time_series_fields` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cae7600d",
   "metadata": {
    "id": "cae7600d"
   },
   "outputs": [],
   "source": [
    "from gluonts.transform.sampler import InstanceSampler\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def create_instance_splitter(\n",
    "    config: PretrainedConfig,\n",
    "    mode: str,\n",
    "    train_sampler: Optional[InstanceSampler] = None,\n",
    "    validation_sampler: Optional[InstanceSampler] = None,\n",
    ") -> Transformation:\n",
    "    assert mode in [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    instance_sampler = {\n",
    "        \"train\": train_sampler\n",
    "        or ExpectedNumInstanceSampler(\n",
    "            num_instances=1.0, min_future=config.prediction_length\n",
    "        ),\n",
    "        \"validation\": validation_sampler\n",
    "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
    "        \"test\": TestSplitSampler(),\n",
    "    }[mode]\n",
    "\n",
    "    return InstanceSplitter(\n",
    "        target_field=\"values\",\n",
    "        is_pad_field=FieldName.IS_PAD,\n",
    "        start_field=FieldName.START,\n",
    "        forecast_start_field=FieldName.FORECAST_START,\n",
    "        instance_sampler=instance_sampler,\n",
    "        past_length=config.context_length + max(config.lags_sequence),\n",
    "        future_length=config.prediction_length,\n",
    "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958e050d",
   "metadata": {
    "id": "958e050d"
   },
   "source": [
    "## Create PyTorch DataLoaders\n",
    "\n",
    "Next, it's time to create PyTorch DataLoaders, which allow us to have batches of (input, output pairs) - or in other words (`past_values`, `future_values`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_lightning in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from pytorch_lightning) (1.24.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from pytorch_lightning) (2.0.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from pytorch_lightning) (4.64.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from pytorch_lightning) (2023.4.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from pytorch_lightning) (0.11.4)\n",
      "Requirement already satisfied: packaging>=17.1 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from pytorch_lightning) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from pytorch_lightning) (4.4.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from pytorch_lightning) (0.8.0)\n",
      "Requirement already satisfied: requests in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.28.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.4)\n",
      "Requirement already satisfied: filelock in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torch>=1.11.0->pytorch_lightning) (3.9.0)\n",
      "Requirement already satisfied: sympy in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torch>=1.11.0->pytorch_lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torch>=1.11.0->pytorch_lightning) (3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from torch>=1.11.0->pytorch_lightning) (3.1.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->pytorch_lightning) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/cs/.pyenv/versions/3.10.6/lib/python3.10/site-packages (from sympy->torch>=1.11.0->pytorch_lightning) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6995101c",
   "metadata": {
    "id": "6995101c"
   },
   "outputs": [],
   "source": [
    "from gluonts.itertools import Cyclic, IterableSlice, PseudoShuffled\n",
    "from gluonts.torch.util import IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def create_train_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    num_batches_per_epoch: int,\n",
    "    shuffle_buffer_length: Optional[int] = None,\n",
    "    **kwargs,\n",
    ") -> Iterable:\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
    "        \"future_values\",\n",
    "        \"future_observed_mask\",\n",
    "    ]\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=True)\n",
    "\n",
    "    # we initialize a Training instance\n",
    "    instance_splitter = create_instance_splitter(config, \"train\") + SelectFields(\n",
    "        TRAINING_INPUT_NAMES\n",
    "    )\n",
    "\n",
    "    # the instance splitter will sample a window of\n",
    "    # context length + lags + prediction length (from the 366 possible transformed time series)\n",
    "    # randomly from within the target time series and return an iterator.\n",
    "    training_instances = instance_splitter.apply(\n",
    "        Cyclic(transformed_data)\n",
    "        if shuffle_buffer_length is None\n",
    "        else PseudoShuffled(\n",
    "            Cyclic(transformed_data),\n",
    "            shuffle_buffer_length=shuffle_buffer_length,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # from the training instances iterator we now return a Dataloader which will\n",
    "    # continue to sample random windows for as long as it is called\n",
    "    # to return batch_size of the appropriate tensors ready for training!\n",
    "    return IterableSlice(\n",
    "        iter(\n",
    "            DataLoader(\n",
    "                IterableDataset(training_instances),\n",
    "                batch_size=batch_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "        ),\n",
    "        num_batches_per_epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10c55455",
   "metadata": {
    "id": "10c55455"
   },
   "outputs": [],
   "source": [
    "def create_test_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=False)\n",
    "\n",
    "    # we create a Test Instance splitter which will sample the very last\n",
    "    # context window seen during training only for the encoder.\n",
    "    instance_sampler = create_instance_splitter(config, \"test\") + SelectFields(\n",
    "        PREDICTION_INPUT_NAMES\n",
    "    )\n",
    "\n",
    "    # we apply the transformations in test mode\n",
    "    testing_instances = instance_sampler.apply(transformed_data, is_train=False)\n",
    "\n",
    "    # This returns a Dataloader which will go over the dataset once.\n",
    "    return DataLoader(\n",
    "        IterableDataset(testing_instances), batch_size=batch_size, **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20e2338b",
   "metadata": {
    "id": "20e2338b"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dataloader = create_train_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_batches_per_epoch=100,\n",
    ")\n",
    "\n",
    "test_dataloader = create_test_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=test_dataset,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ES2U8g-2G2Jd",
   "metadata": {
    "id": "ES2U8g-2G2Jd"
   },
   "source": [
    "Let's check the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "YU2h9OOB5IsX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YU2h9OOB5IsX",
    "outputId": "ba8073bb-f450-4cd1-faee-3fb129f35c7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past_time_features torch.Size([256, 49, 2]) torch.FloatTensor\n",
      "past_values torch.Size([256, 49]) torch.FloatTensor\n",
      "past_observed_mask torch.Size([256, 49]) torch.FloatTensor\n",
      "future_time_features torch.Size([256, 15, 2]) torch.FloatTensor\n",
      "static_categorical_features torch.Size([256, 1]) torch.LongTensor\n",
      "future_values torch.Size([256, 15]) torch.FloatTensor\n",
      "future_observed_mask torch.Size([256, 15]) torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape, v.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HvvPlSF8HBYd",
   "metadata": {
    "id": "HvvPlSF8HBYd"
   },
   "source": [
    "As can be seen, we don't feed `input_ids` and `attention_mask` to the encoder (as would be the case for NLP models), but rather `past_values`, along with `past_observed_mask`, `past_time_features`, and `static_categorical_features`.\n",
    "\n",
    "The decoder inputs consist of `future_values`, `future_observed_mask` and `future_time_features`. The `future_values` can be seen as the equivalent of `decoder_input_ids` in NLP.\n",
    "\n",
    "We refer to the [docs](https://huggingface.co/docs/transformers/model_doc/time_series_transformer#transformers.TimeSeriesTransformerForPrediction.forward.past_values) for a detailed explanation for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_cev4ufVv1yf",
   "metadata": {
    "id": "_cev4ufVv1yf"
   },
   "source": [
    "## Forward pass\n",
    "\n",
    "Let's perform a single forward pass with the batch we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sD8fav6qTApR",
   "metadata": {
    "id": "sD8fav6qTApR"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[32], expected input with shape [*, 32], but got input of size[256, 30, 18]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m## cs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# perform forward pass\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_time_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_observed_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatic_categorical_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_static_categorical_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatic_real_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_static_real_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m## cs: changed from \"else None\"\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfuture_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfuture_time_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfuture_observed_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/transformers/models/time_series_transformer/modeling_time_series_transformer.py:1820\u001b[0m, in \u001b[0;36mTimeSeriesTransformerForPrediction.forward\u001b[0;34m(self, past_values, past_time_features, past_observed_mask, static_categorical_features, static_real_features, future_values, future_time_features, future_observed_mask, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, output_hidden_states, output_attentions, use_cache, return_dict)\u001b[0m\n\u001b[1;32m   1817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m future_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1818\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1820\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1835\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1836\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1838\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1840\u001b[0m prediction_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1841\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/transformers/models/time_series_transformer/modeling_time_series_transformer.py:1645\u001b[0m, in \u001b[0;36mTimeSeriesTransformerModel.forward\u001b[0;34m(self, past_values, past_time_features, past_observed_mask, static_categorical_features, static_real_features, future_values, future_time_features, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, output_hidden_states, output_attentions, use_cache, return_dict)\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1644\u001b[0m     enc_input \u001b[38;5;241m=\u001b[39m transformer_inputs[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcontext_length, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m-> 1645\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/transformers/models/time_series_transformer/modeling_time_series_transformer.py:1118\u001b[0m, in \u001b[0;36mTimeSeriesTransformerEncoder.forward\u001b[0;34m(self, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1115\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1117\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[0;32m-> 1118\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# expand attention_mask\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[32], expected input with shape [*, 32], but got input of size[256, 30, 18]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") ## cs\n",
    "\n",
    "# perform forward pass\n",
    "outputs = model(\n",
    "    past_values=batch[\"past_values\"],\n",
    "    past_time_features=batch[\"past_time_features\"],\n",
    "    past_observed_mask=batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=batch[\"static_categorical_features\"]\n",
    "    if config.num_static_categorical_features > 0\n",
    "    else None,\n",
    "    static_real_features=batch[\"static_real_features\"]\n",
    "    if config.num_static_real_features > 0\n",
    "    else torch.zeros((batch_size, 0), dtype=torch.float32, device=device),  ## cs: changed from \"else None\"\n",
    "    future_values=batch[\"future_values\"],\n",
    "    future_time_features=batch[\"future_time_features\"],\n",
    "    future_observed_mask=batch[\"future_observed_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j2KnnHTCX4RC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j2KnnHTCX4RC",
    "outputId": "7b11e3b4-c4e1-48fd-c93e-69a21f28e072"
   },
   "outputs": [],
   "source": [
    "print(\"Loss:\", outputs.loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V9K8s2j9y8x7",
   "metadata": {
    "id": "V9K8s2j9y8x7"
   },
   "source": [
    "Note that the model is returning a loss. This is possible as the decoder automatically shifts the `future_values` one position to the right in order to have the labels. This allows computing a loss between the predicted values and the labels.\n",
    "\n",
    "Also note that the decoder uses a causal mask to not look into the future as the values it needs to predict are in the `future_values` tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SxHDCa7vwPBF",
   "metadata": {
    "id": "SxHDCa7vwPBF"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "It's time to train the model! We'll use a standard PyTorch training loop.\n",
    "\n",
    "We will use the ðŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index) library here, which automatically places the model, optimizer and dataloader on the appropriate `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gMLYvQaNHuXQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMLYvQaNHuXQ",
    "outputId": "ee7ecc34-8c6f-4fc9-9dc6-aeb327822373",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\n",
    "\n",
    "model, optimizer, train_dataloader = accelerator.prepare(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(40):\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else None,\n",
    "            static_real_features=batch[\"static_real_features\"].to(device)\n",
    "            if config.num_static_real_features > 0\n",
    "            else None,\n",
    "            past_time_features=batch[\"past_time_features\"].to(device),\n",
    "            past_values=batch[\"past_values\"].to(device),\n",
    "            future_time_features=batch[\"future_time_features\"].to(device),\n",
    "            future_values=batch[\"future_values\"].to(device),\n",
    "            past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "            future_observed_mask=batch[\"future_observed_mask\"].to(device),\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q1U6YhaFXlSp",
   "metadata": {
    "id": "Q1U6YhaFXlSp"
   },
   "source": [
    "## Inference\n",
    "\n",
    "At inference time, it's recommended to use the `generate()` method for autoregressive generation, similar to NLP models.\n",
    "\n",
    "Forecasting involves getting data from the test instance sampler, which will sample the very last `context_length` sized window of values from each time series in the dataset, and pass it to the model. Note that we pass `future_time_features`, which are known ahead of time, to the decoder.\n",
    "\n",
    "The model will autoregressively sample a certain number of values from the predicted distribution and pass them back to the decoder to return the prediction outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7482c1",
   "metadata": {
    "id": "7c7482c1"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "forecasts = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    outputs = model.generate(\n",
    "        static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "        if config.num_static_categorical_features > 0\n",
    "        else None,\n",
    "        static_real_features=batch[\"static_real_features\"].to(device)\n",
    "        if config.num_static_real_features > 0\n",
    "        else None,\n",
    "        past_time_features=batch[\"past_time_features\"].to(device),\n",
    "        past_values=batch[\"past_values\"].to(device),\n",
    "        future_time_features=batch[\"future_time_features\"].to(device),\n",
    "        past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "    )\n",
    "    forecasts.append(outputs.sequences.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kPLiRcOeZR67",
   "metadata": {
    "id": "kPLiRcOeZR67"
   },
   "source": [
    "The model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`). \n",
    "\n",
    "In this case, we get `100` possible values for the next `24` months (for each example in the batch which is of size `64`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DwAfSZitZNAQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwAfSZitZNAQ",
    "outputId": "8ca71318-2361-4c9c-ae93-88ba9a453fef"
   },
   "outputs": [],
   "source": [
    "forecasts[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fCTBw_t91xwH",
   "metadata": {
    "id": "fCTBw_t91xwH"
   },
   "source": [
    "We'll stack them vertically, to get forecasts for all time-series in the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "du1GyJVXlpHp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "du1GyJVXlpHp",
    "outputId": "cfd3e777-da77-4ca6-eedd-e0d0f22da872"
   },
   "outputs": [],
   "source": [
    "forecasts = np.vstack(forecasts)\n",
    "print(forecasts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlvFCdgiA9oy",
   "metadata": {
    "id": "wlvFCdgiA9oy"
   },
   "source": [
    "We can evaluate the resulting forecast with respect to the ground truth out of sample values present in the test set. For that, we'll use the ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) library, which includes the [MASE](https://huggingface.co/spaces/evaluate-metric/mase) and [sMAPE](https://huggingface.co/spaces/evaluate-metric/smape) metrics.\n",
    "\n",
    "We calculate both metrics for each time series in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0yb9RnczYE4z",
   "metadata": {
    "id": "0yb9RnczYE4z"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "mase_metric = load(\"evaluate-metric/mase\")\n",
    "smape_metric = load(\"evaluate-metric/smape\")\n",
    "\n",
    "forecast_median = np.median(forecasts, 1)\n",
    "\n",
    "mase_metrics = []\n",
    "smape_metrics = []\n",
    "for item_id, ts in enumerate(test_dataset):\n",
    "    training_data = ts[\"target\"][:-prediction_length]\n",
    "    ground_truth = ts[\"target\"][-prediction_length:]\n",
    "    mase = mase_metric.compute(\n",
    "        predictions=forecast_median[item_id],\n",
    "        references=np.array(ground_truth),\n",
    "        training=np.array(training_data),\n",
    "        periodicity=get_seasonality(freq),\n",
    "    )\n",
    "    mase_metrics.append(mase[\"mase\"])\n",
    "\n",
    "    smape = smape_metric.compute(\n",
    "        predictions=forecast_median[item_id],\n",
    "        references=np.array(ground_truth),\n",
    "    )\n",
    "    smape_metrics.append(smape[\"smape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuuFbNYdZlIR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fuuFbNYdZlIR",
    "outputId": "9a010645-30fc-4af8-bc53-636391ba0ec6"
   },
   "outputs": [],
   "source": [
    "print(f\"MASE: {np.mean(mase_metrics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w6ETpUrML2wE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6ETpUrML2wE",
    "outputId": "77127e13-36b1-4cd2-9f90-63f3f23c6db0"
   },
   "outputs": [],
   "source": [
    "print(f\"sMAPE: {np.mean(smape_metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S91QglbEL7Qm",
   "metadata": {
    "id": "S91QglbEL7Qm"
   },
   "source": [
    "We can also plot the individual metrics of each time series in the dataset and observe that a handful of time series contribute a lot to the final test metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb342aac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "cb342aac",
    "outputId": "445d7c09-a428-4618-d13a-bca903bbc465"
   },
   "outputs": [],
   "source": [
    "plt.scatter(mase_metrics, smape_metrics, alpha=0.3)\n",
    "plt.xlabel(\"MASE\")\n",
    "plt.ylabel(\"sMAPE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moksM2QmMACr",
   "metadata": {
    "id": "moksM2QmMACr"
   },
   "source": [
    "To plot the prediction for any time series with respect the ground truth test data we define the following helper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae05011",
   "metadata": {
    "id": "3ae05011"
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "def plot(ts_index):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    index = pd.period_range(\n",
    "        start=test_dataset[ts_index][FieldName.START],\n",
    "        periods=len(test_dataset[ts_index][FieldName.TARGET]),\n",
    "        freq=freq,\n",
    "    ).to_timestamp()\n",
    "\n",
    "    # Major ticks every half year, minor ticks every month,\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 7)))\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "    ax.plot(\n",
    "        index[-2 * prediction_length :],\n",
    "        test_dataset[ts_index][\"target\"][-2 * prediction_length :],\n",
    "        label=\"actual\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        index[-prediction_length:],\n",
    "        np.median(forecasts[ts_index], axis=0),\n",
    "        label=\"median\",\n",
    "    )\n",
    "\n",
    "    plt.fill_between(\n",
    "        index[-prediction_length:],\n",
    "        forecasts[ts_index].mean(0) - forecasts[ts_index].std(axis=0),\n",
    "        forecasts[ts_index].mean(0) + forecasts[ts_index].std(axis=0),\n",
    "        alpha=0.3,\n",
    "        interpolate=True,\n",
    "        label=\"+/- 1-std\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mwtksAcxMHoK",
   "metadata": {
    "id": "mwtksAcxMHoK"
   },
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5N8fdnm_MKQP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "5N8fdnm_MKQP",
    "outputId": "3b221c31-ee20-4626-9592-2492ce143b4a"
   },
   "outputs": [],
   "source": [
    "plot(334)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nf4Y8MvLMOi8",
   "metadata": {
    "id": "Nf4Y8MvLMOi8"
   },
   "source": [
    "How do we compare against other models? The [Monash Time Series Repository](https://forecastingdata.org/#results) has a comparison table of test set MASE metrics which we can add to:\n",
    "\n",
    "|Dataset | \tSES| \tTheta | \tTBATS| \tETS\t| (DHR-)ARIMA| \tPR|\tCatBoost |\tFFNN\t| DeepAR | \tN-BEATS | \tWaveNet| \t**Transformer** (Our) |\n",
    "|:------------------:|:-----------------:|:--:|:--:|:--:|:--:|:--:|:--:|:---:|:---:|:--:|:--:|:--:|\n",
    "|Tourism Monthly | \t3.306 |\t1.649 |\t1.751 |\t1.526|\t1.589|\t1.678\t|1.699|\t1.582\t| 1.409\t| 1.574|\t1.482\t|  **1.256**|\n",
    "\n",
    "Note that, with our model, we are beating all other models reported (see also table 2 in the corresponding [paper](https://openreview.net/pdf?id=wEc1mgAjU-)), and we didn't do any hyperparameter tuning. We just trained the Transformer for 40 epochs. \n",
    "\n",
    "Of course, we need to be careful with just claiming state-of-the-art results on time series with neural networks, as it seems [\"XGBoost is typically all you need\"](https://www.sciencedirect.com/science/article/pii/S0169207021001679).  We are just very curious to see how far neural networks can bring us, and whether Transformers are going to be useful in this domain. This particular dataset seems to indicate that it's definitely worth exploring.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "We would encourage the readers to try out the [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) with other time series datasets from the [Hub](https://huggingface.co/datasets/monash_tsf) and replace the appropriate frequency and prediction length parameters. For your datasets, one would need to convert them to the convention used by GluonTS, which is explained nicely in their documentation [here](https://ts.gluon.ai/stable/tutorials/forecasting/extended_tutorial.html#What-is-in-a-dataset?). We have also prepared an example notebook showing you how to convert your dataset into the ðŸ¤— datasets format [here](https://github.com/huggingface/notebooks/blob/main/examples/time_series_datasets.ipynb).\n",
    "\n",
    "As time series researchers will know, there has been a lot of interest in applying Transformer based models to the time series problem. The vanilla Transformer is just one of many attention-based models and so there is a need to add more models to the library.\n",
    "\n",
    "At the moment there is nothing stopping us from modeling multivariate time series, however for that one would need to instantiate the model with a multivariate distribution head. Currently, diagonal independent distributions are supported, and other multivariate distributions will be added. Stay tuned for a future blog post which will include a tutorial.\n",
    "\n",
    "Another thing on the roadmap is time series classification. This entails adding a time series model with a classification head to the library, for the anomaly detection task for example. \n",
    "\n",
    "The current model assumes the presence of a date-time together with the time series values, which might not be the case for every time series in the wild. See for instance neuroscience datasets like the one from [WOODS](https://woods-benchmarks.github.io/). Thus, one would need to generalize the current model to make some inputs optional in the whole pipeline.\n",
    "\n",
    "Finally, the NLP/Vision domain has benefitted tremendously from [large pre-trained models](https://arxiv.org/abs/1810.04805), while this is not the case as far as we are aware for the time series domain. Transformer based models seem like the obvious choice in pursuing this avenue of research and we cannot wait to see what researchers and practitioners come up with!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CYtLcclMTs99",
   "metadata": {
    "id": "CYtLcclMTs99"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
